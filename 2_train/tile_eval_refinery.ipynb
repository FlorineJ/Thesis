{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor, hooks\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset, print_csv_format\n",
    "\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import detectron2\n",
    "import pandas as pd\n",
    "\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from train import get_dataset_dicts, get_dataset_dicts_with_regions, random_visu, setup_cfg, MyTrainer, load_json_arr, find_best_model\n",
    "\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOLDER PATH ###\n",
    "\n",
    "dataset_eval = \"test\"\n",
    "dataset_eval_output = \"train_output_refinery_data\"\n",
    "annotation_json = \"via_region_data_refinery_annotations.json\"\n",
    "results_file = \"results.json\"\n",
    "experiment = \"../Satellite/output/lr001_BS4_refinery_test__aug_1\"\n",
    "conf_path = \"../Satellite/output/lr001_BS4_refinery_test__aug_1/NSOD2cfg_1000_169r_1000pix_noBT_lr001_BS4_refinery_test__aug_1.yaml\"\n",
    "model_path = experiment + \"/model_51.pth\"\n",
    "filter_empty_annot = False\n",
    "\n",
    "\n",
    "annotation_output = os.path.splitext(annotation_json)[0]\n",
    "out_dir = os.path.join(experiment, dataset_eval_output, annotation_output)\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    print (\"The test directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dicts(img_dir, annotation_json):\n",
    "    with open(os.path.join(img_dir, annotation_json)) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(annotations.values()):\n",
    "        record = {}\n",
    "        filename = os.path.join(img_dir, v[\"filename\"])\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        \n",
    "        annos = v[\"regions\"]\n",
    "        objs = []\n",
    "        for anno_key, anno in annos.items():\n",
    "            shape_attr = anno[\"shape_attributes\"]\n",
    "            region_attr = anno[\"region_attributes\"]\n",
    "            \n",
    "            # Ensure that region_attributes is processed correctly\n",
    "            if \"type\" not in region_attr or region_attr[\"type\"] != \"refinery\":\n",
    "                continue  # Skip non-container regions if any\n",
    "\n",
    "            px = shape_attr[\"all_points_x\"]\n",
    "            py = shape_attr[\"all_points_y\"]\n",
    "            poly = [(x, y) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0,  # Assuming 'container' is the only category\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Indicate to D2 how to read dataset if not in coco format: ###\n",
    "\n",
    "# D2 metadata: name of classes and colors of annotations\n",
    "classes = [\"refinery\"]\n",
    "colors = [(249, 180, 45)]\n",
    "\n",
    "# Register dataset and metadata\n",
    "for d in [\"train\", \"val\", \"test\"]:\n",
    "    DatasetCatalog.register(d, lambda d=d:\n",
    "                            get_dataset_dicts(os.path.join(\"../Satellite\", d), annotation_json))\n",
    "    # Key-value mapping to interpret whatâ€™s in the dataset: names of classes, colors of classes\n",
    "    MetadataCatalog.get(d).thing_classes = classes\n",
    "    MetadataCatalog.get(d).thing_colors = colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "# keep those 2 to avoid errors of MIN_SIZE_TRAIN = 800 and resize not found\n",
    "#change in config: \n",
    "#MIN_SIZE_TRAIN: !!python/tuple\n",
    "#- 1000\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (1000,)\n",
    "cfg.RESIZE= False\n",
    "cfg.merge_from_file(conf_path)\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (1000,)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = model_path\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = filter_empty_annot\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "evaluator = COCOEvaluator(dataset_eval, output_dir=out_dir)\n",
    "\n",
    "val_loader = build_detection_test_loader(cfg, dataset_eval)\n",
    "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "\n",
    "with open(os.path.join(out_dir, results_file), 'w') as f:\n",
    "    json.dump(results, f)\n",
    "print(\"Results dumped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_dir = '../NSO/output/lr001_BS4_empty-annot-50/eval'\n",
    "# Load the JSON pred file into a DataFrame\n",
    "df_pred = pd.read_json(os.path.join(out_dir, 'coco_instances_results.json'))\n",
    "df_pred = df_pred[df_pred['score'] > 0.5] \n",
    "\n",
    "# Load the JSON ground truth file into a DataFrame\n",
    "\n",
    "# CHANGE VAL OR TEST\n",
    "# Extract the \"images\" key and create a DataFrame from it\n",
    "\n",
    "# Ground thruth OG\n",
    "df_truth_og = pd.read_json(os.path.join(os.path.join(\"../Satellite\", dataset_eval), annotation_json))\n",
    "with open(os.path.join(out_dir, 'test_coco_format.json'), 'r') as j:\n",
    "     data = json.loads(j.read())\n",
    "images_data = data.get('images', [])  # Get the list of \"images\" objects\n",
    "annotation_data = data.get('annotations', [])  # Get the list of \"categories\" objects\n",
    "# Create a DataFrame from the valid \"images\" objects\n",
    "df_images = pd.DataFrame(images_data)\n",
    "df_annotations = pd.DataFrame(annotation_data)\n",
    "#Drop duplicates: where are they comming from???? =>val_coco is automatically done by D2 check if where nitially in dataset\n",
    "df_annotations = df_annotations.drop_duplicates(subset=['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth = pd.merge(df_images, df_annotations, left_on='id', right_on='image_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop duplicates => why are they there?\n",
    "#df_truth = df_truth.drop_duplicates(subset=['bbox'])\n",
    "df_eval = pd.merge(df_truth, df_pred, left_on='id_x', right_on='image_id', how='left')\n",
    "# df_eval = df_eval.drop_duplicates(subset=['segmentation_x'])\n",
    "# #draop images with several annotation, just need to keep one\n",
    "df_eval = df_eval.drop_duplicates(subset=['id_x'])\n",
    "df_eval['truth'] = None\n",
    "df_eval['pred'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['truth'] = np.where(df_eval.bbox_x.isnull(), 0, 1)\n",
    "df_eval['pred'] = np.where(df_eval.score.isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#ravel to flatten conf matrix to 1D insted of 2D\n",
    "tn, fp, fn, tp = confusion_matrix(df_eval['truth'], df_eval['pred']).ravel()\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "print(\"Precision: \", precision_score)\n",
    "print(\"Recall: \", recall_score)\n",
    "print (\"Fscore: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert int64 values to native Python int\n",
    "tn = int(tn)\n",
    "fp = int(fp)\n",
    "fn = int(fn)\n",
    "tp = int(tp)\n",
    "\n",
    "metrics_dict = {\n",
    "    'tn': tn,\n",
    "    'fp': fp,\n",
    "    'fn': fn,\n",
    "    'tp': tp,\n",
    "    'precision_score': precision_score,\n",
    "    'recall_score': recall_score,\n",
    "    'f1_score': f1_score\n",
    "}\n",
    "\n",
    "# Define the file path\n",
    "json_metrics = 'metrics_tiles.json'\n",
    "json_path = os.path.join(out_dir, json_metrics)\n",
    "\n",
    "# Save the metrics to a JSON file\n",
    "with open(json_path, 'w') as json_file:\n",
    "    json.dump(metrics_dict, json_file, indent=4)\n",
    "\n",
    "print(f'Metrics saved to {json_file}')\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(df_eval['truth'], df_eval['pred'])\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix with enhanced visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.4)  # Adjust to increase the size of the labels\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Predicted: Not Refinery', 'Predicted: Refinery'],\n",
    "            yticklabels=['Actual: Not Refinery', 'Actual: Refinery'])\n",
    "plt.xlabel('Prediction', fontsize=16)\n",
    "plt.ylabel('Truth', fontsize=16)\n",
    "plt.title('Confusion Matrix', fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "import random\n",
    "\n",
    "# Function to load image\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "    return image\n",
    "\n",
    "# Function to visualize ground truth and predictions\n",
    "def visualize_predictions(image_path, ground_truth, predictions):\n",
    "    image = load_image(image_path)\n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    visualizer = Visualizer(image[:, :, ::-1], MetadataCatalog.get(dataset_eval), scale=1.2)\n",
    "    \n",
    "    # Draw ground truth\n",
    "    if ground_truth is not None:\n",
    "        for gt in ground_truth:\n",
    "            bbox = gt['bbox_x']\n",
    "            # Check if bbox is correctly formatted\n",
    "            if len(bbox) != 4:\n",
    "                print(f\"Invalid Ground Truth BBox format: {bbox}\")\n",
    "                continue\n",
    "            bbox_mode = BoxMode.convert(bbox, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)  # Assuming bbox is in XYWH format\n",
    "            print(f\"Ground Truth BBox: {bbox_mode}\")\n",
    "            visualizer.draw_box(bbox_mode, edge_color=(0, 1, 0, 1))  # RGBA values within 0-1 range\n",
    "            visualizer.draw_text(\"GT\", bbox_mode[:2], color='g', font_size=10)\n",
    "    \n",
    "    # Draw predictions\n",
    "    if predictions is not None:\n",
    "        for pred in predictions:\n",
    "            bbox = pred['bbox_y']\n",
    "            # Check if bbox is correctly formatted\n",
    "            if len(bbox) != 4:\n",
    "                print(f\"Invalid Prediction BBox format: {bbox}\")\n",
    "                continue\n",
    "            bbox_mode = BoxMode.convert(bbox, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)  # Assuming bbox is already in XYXY format\n",
    "            score = pred['score']\n",
    "            print(f\"Prediction BBox: {bbox_mode}, Score: {score}\")\n",
    "            visualizer.draw_box(bbox_mode, edge_color=(1, 0, 0, 1))  # RGBA values within 0-1 range\n",
    "            visualizer.draw_text(f\"Pred: {score:.2f}\", bbox_mode[:2], color='r', font_size=10)\n",
    "    \n",
    "    vis_image = visualizer.get_output().get_image()[:, :, ::-1]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(vis_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ensure dataset_eval is defined\n",
    "dataset_eval = \"test\"  # Change this to the appropriate dataset name if different\n",
    "\n",
    "# Visualize 10 random examples\n",
    "image_dir = \"../Satellite/test\"  # Correct base path for the test images\n",
    "unique_image_names = df_eval['file_name'].unique()\n",
    "random_images = random.sample(list(unique_image_names), 10)\n",
    "\n",
    "for full_image_name in random_images:\n",
    "    # Get image file path\n",
    "    image_path = full_image_name\n",
    "    \n",
    "    # Print the file path to ensure correctness\n",
    "    print(f\"Image Path: {image_path}\")\n",
    "    \n",
    "    # Get ground truth and predictions for the specific image using the full path\n",
    "    gt_info = df_eval[df_eval['file_name'] == full_image_name]\n",
    "    print(f\"Ground truth records found: {len(gt_info)}\")\n",
    "    \n",
    "    ground_truth = gt_info[~gt_info['bbox_x'].isnull()][['bbox_x', 'category_id_x']].to_dict('records')\n",
    "    \n",
    "    # Print ground truth information for debugging\n",
    "    print(\"Ground Truth Info:\")\n",
    "    for gt in ground_truth:\n",
    "        print(gt)\n",
    "    \n",
    "    pred_info = df_eval[df_eval['file_name'] == full_image_name]\n",
    "    print(f\"Prediction records found: {len(pred_info)}\")\n",
    "    \n",
    "    predictions = pred_info[~pred_info['bbox_y'].isnull()][['bbox_y', 'score', 'category_id_y']].to_dict('records')\n",
    "    \n",
    "    # Print prediction information for debugging\n",
    "    print(\"Prediction Info:\")\n",
    "    for pred in predictions:\n",
    "        print(pred)\n",
    "    \n",
    "    # Visualize predictions on the image\n",
    "    visualize_predictions(image_path, ground_truth, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "import random\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "# Function to load image\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "    return image\n",
    "\n",
    "# Function to convert polygon segmentation to binary mask\n",
    "def polygon_to_mask(segmentation, height, width):\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    points = np.array(segmentation).reshape(-1, 2)\n",
    "    cv2.fillPoly(mask, [points], 1)\n",
    "    return mask\n",
    "\n",
    "# Function to visualize ground truth and predictions with segmentation masks\n",
    "def visualize_predictions(image_path, ground_truth, predictions):\n",
    "    image = load_image(image_path)\n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    visualizer = Visualizer(image[:, :, ::-1], MetadataCatalog.get(dataset_eval), scale=1.2)\n",
    "    \n",
    "    # Draw ground truth masks\n",
    "    if ground_truth is not None:\n",
    "        for gt in ground_truth:\n",
    "            segmentation = gt['segmentation_x']\n",
    "            print(f\"Ground Truth Segmentation: {segmentation}\")\n",
    "            mask = polygon_to_mask(segmentation, image.shape[0], image.shape[1])\n",
    "            visualizer.draw_binary_mask(mask, color=(0, 1, 0, 0.3), edge_color=(0, 1, 0, 1))\n",
    "            visualizer.draw_text(\"GT\", (segmentation[0][0], segmentation[0][1]), color='g', font_size=10)\n",
    "    \n",
    "    # Draw prediction masks\n",
    "    if predictions is not None:\n",
    "        for pred in predictions:\n",
    "            segmentation = pred['segmentation_y']\n",
    "            score = pred['score']\n",
    "            print(f\"Prediction Segmentation: {segmentation}, Score: {score}\")\n",
    "            mask = mask_util.decode(segmentation)\n",
    "            visualizer.draw_binary_mask(mask, color=(1, 0, 0, 0.3), edge_color=(1, 0, 0, 1))\n",
    "            visualizer.draw_text(f\"Pred: {score:.2f}\", (mask.shape[1] // 2, mask.shape[0] // 2), color='r', font_size=10)\n",
    "    \n",
    "    vis_image = visualizer.get_output().get_image()[:, :, ::-1]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(vis_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ensure dataset_eval is defined\n",
    "dataset_eval = \"test\"  # Change this to the appropriate dataset name if different\n",
    "\n",
    "# Visualize 10 random examples\n",
    "image_dir = \"../Satellite/test\"  # Correct base path for the test images\n",
    "unique_image_names = df_eval['file_name'].unique()\n",
    "random_images = random.sample(list(unique_image_names), 10)\n",
    "\n",
    "for full_image_name in random_images:\n",
    "    # Get image file path\n",
    "    image_path = full_image_name\n",
    "    \n",
    "    # Print the file path to ensure correctness\n",
    "    print(f\"Image Path: {image_path}\")\n",
    "    \n",
    "    # Get ground truth and predictions for the specific image using the full path\n",
    "    gt_info = df_eval[df_eval['file_name'] == full_image_name]\n",
    "    print(f\"Ground truth records found: {len(gt_info)}\")\n",
    "    \n",
    "    ground_truth = gt_info[~gt_info['segmentation_x'].isnull()][['segmentation_x', 'category_id_x']].to_dict('records')\n",
    "    \n",
    "    # Print ground truth information for debugging\n",
    "    print(\"Ground Truth Info:\")\n",
    "    for gt in ground_truth:\n",
    "        print(gt)\n",
    "    \n",
    "    pred_info = df_eval[df_eval['file_name'] == full_image_name]\n",
    "    print(f\"Prediction records found: {len(pred_info)}\")\n",
    "    \n",
    "    predictions = pred_info[~pred_info['segmentation_y'].isnull()][['segmentation_y', 'score', 'category_id_y']].to_dict('records')\n",
    "    \n",
    "    # Print prediction information for debugging\n",
    "    print(\"Prediction Info:\")\n",
    "    for pred in predictions:\n",
    "        print(pred)\n",
    "    \n",
    "    # Visualize predictions on the image\n",
    "    visualize_predictions(image_path, ground_truth, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for overlaps where truth and pred are both 1\n",
    "overlaps_truth_pred_1 = df_eval[(df_eval['truth'] == 1) & (df_eval['pred'] == 1)]\n",
    "\n",
    "# Filter for non-overlaps where truth is 1 and pred is 0\n",
    "non_overlaps_truth_1_pred_0 = df_eval[(df_eval['truth'] == 1) & (df_eval['pred'] == 0)]\n",
    "\n",
    "# Filter for truth == 0 and pred == 1\n",
    "truth_0_pred_1 = df_eval[(df_eval['truth'] == 0) & (df_eval['pred'] == 1)]\n",
    "\n",
    "# Filter for truth == 0 and pred == 0\n",
    "truth_0_pred_0 = df_eval[(df_eval['truth'] == 0) & (df_eval['pred'] == 0)]\n",
    "\n",
    "# Extract unique image IDs for overlaps (truth == 1 and pred == 1)\n",
    "overlap_image_ids = overlaps_truth_pred_1['file_name'].unique()\n",
    "\n",
    "# Extract unique image IDs for non-overlaps (truth == 1 and pred == 0)\n",
    "non_overlap_image_ids = non_overlaps_truth_1_pred_0['file_name'].unique()\n",
    "\n",
    "# Extract unique image IDs for truth == 0 and pred == 1\n",
    "truth_0_pred_1_image_ids = truth_0_pred_1['file_name'].unique()\n",
    "\n",
    "# Extract unique image IDs for truth == 0 and pred == 0\n",
    "truth_0_pred_0_image_ids = truth_0_pred_0['file_name'].unique()\n",
    "\n",
    "# Print the results with counts\n",
    "print(\"Image IDs where truth and prediction overlap (truth == 1 and pred == 1):\")\n",
    "print(overlap_image_ids)\n",
    "print(f\"Count: {len(overlap_image_ids)}\\n\")\n",
    "\n",
    "print(\"Image IDs where truth is 1 and prediction is 0:\")\n",
    "print(non_overlap_image_ids)\n",
    "print(f\"Count: {len(non_overlap_image_ids)}\\n\")\n",
    "\n",
    "print(\"Image IDs where truth is 0 and prediction is 1:\")\n",
    "print(truth_0_pred_1_image_ids)\n",
    "print(f\"Count: {len(truth_0_pred_1_image_ids)}\\n\")\n",
    "\n",
    "print(\"Image IDs where truth is 0 and prediction is 0:\")\n",
    "print(truth_0_pred_0_image_ids)\n",
    "print(f\"Count: {len(truth_0_pred_0_image_ids)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util\n",
    "import cv2\n",
    "\n",
    "# Function to convert polygon segmentation to binary mask\n",
    "def polygon_to_mask(segmentation, height, width):\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    points = np.array(segmentation).reshape(-1, 2)\n",
    "    cv2.fillPoly(mask, [points], 1)\n",
    "    return mask\n",
    "\n",
    "# Function to plot a single image with annotations\n",
    "def plot_single_image(tile_path, annotation_path, df_eval, ax, title, is_prediction=False):\n",
    "    # Load the image\n",
    "    image = Image.open(tile_path)\n",
    "    \n",
    "    # Load the annotation JSON file\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Extract the filename from the tile path\n",
    "    filename = tile_path.split('/')[-1]\n",
    "    \n",
    "    # Get the annotations for the specified tile\n",
    "    tile_annotations = annotations.get(filename, {}).get('regions', {})\n",
    "    \n",
    "    # Get predictions from df_eval\n",
    "    predictions = df_eval[df_eval['file_name'] == tile_path].to_dict('records')\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Function to plot regions\n",
    "    def plot_regions(ax, regions, edgecolor, facecolor, label_suffix):\n",
    "        for region in regions.values():\n",
    "            shape_attributes = region.get('shape_attributes', {})\n",
    "            region_attributes = region.get('region_attributes', {})\n",
    "            \n",
    "            if shape_attributes.get('name') == 'polygon':\n",
    "                all_points_x = shape_attributes.get('all_points_x', [])\n",
    "                all_points_y = shape_attributes.get('all_points_y', [])\n",
    "                polygon = list(zip(all_points_x, all_points_y))\n",
    "                \n",
    "                # Determine color based on type\n",
    "                annotation_type = region_attributes.get('type', 'unknown')\n",
    "                if annotation_type == 'refinery':\n",
    "                    edgecolor = 'g'\n",
    "                    facecolor = 'green'\n",
    "                else:\n",
    "                    edgecolor = 'r'\n",
    "                    facecolor = 'red'\n",
    "                \n",
    "                # Create a polygon patch with a semi-transparent fill color\n",
    "                poly_patch = patches.Polygon(polygon, closed=True, edgecolor=edgecolor, facecolor=facecolor, alpha=0.5)\n",
    "                ax.add_patch(poly_patch)\n",
    "                \n",
    "                # Get the centroid of the polygon to place the label\n",
    "                centroid_x = sum(all_points_x) / len(all_points_x)\n",
    "                centroid_y = sum(all_points_y) / len(all_points_y)\n",
    "                \n",
    "                # Get the type/category for the annotation\n",
    "                annotation_type = region_attributes.get('type', 'unknown')\n",
    "                \n",
    "                # Add text annotation to the plot\n",
    "                ax.text(centroid_x, centroid_y, f\"{annotation_type} {label_suffix}\", color='white', fontsize=12, ha='center', va='center',\n",
    "                        bbox=dict(facecolor=edgecolor, alpha=0.5, edgecolor='none', pad=1))\n",
    "    \n",
    "    # Plot ground truth annotations\n",
    "    if not is_prediction:\n",
    "        plot_regions(ax, tile_annotations, edgecolor='r', facecolor='red', label_suffix='(GT)')\n",
    "    \n",
    "    # Plot predictions from df_eval\n",
    "    if is_prediction:\n",
    "        for pred in predictions:\n",
    "            if 'segmentation_y' in pred and pred['segmentation_y']:\n",
    "                segmentation = pred['segmentation_y']\n",
    "                score = pred['score']\n",
    "                if isinstance(segmentation, dict) and 'size' in segmentation and 'counts' in segmentation:  # Check if it's a RLE\n",
    "                    mask = mask_util.decode(segmentation)\n",
    "                    poly_patch = patches.Polygon(np.argwhere(mask).reshape(-1, 2), closed=True, edgecolor='b', facecolor='blue', alpha=0.5)\n",
    "                    ax.add_patch(poly_patch)\n",
    "                    centroid_x, centroid_y = np.mean(np.argwhere(mask), axis=0)\n",
    "                else:\n",
    "                    # Handle the polygon segmentation case\n",
    "                    if isinstance(segmentation, list):\n",
    "                        segmentation = np.array(segmentation).reshape(-1, 2)\n",
    "                        poly_patch = patches.Polygon(segmentation, closed=True, edgecolor='b', facecolor='blue', alpha=0.5)\n",
    "                        ax.add_patch(poly_patch)\n",
    "                        centroid_x = np.mean(segmentation[:, 0])\n",
    "                        centroid_y = np.mean(segmentation[:, 1])\n",
    "                    else:\n",
    "                        continue\n",
    "                # Add text annotation to the plot\n",
    "                ax.text(centroid_x, centroid_y, f\"Pred: {score:.2f}\", color='white', fontsize=12, ha='center', va='center',\n",
    "                        bbox=dict(facecolor='blue', alpha=0.5, edgecolor='none', pad=1))\n",
    "\n",
    "# Example data\n",
    "image_paths_overlap_1 = [\n",
    "    '../Satellite/test/301_Rotterdam_19000_43000.png',\n",
    "     '../Satellite/test/352_Goteborg_11000_8000.png',\n",
    "     '../Satellite/test/349_Vlores_20000_12000.png',\n",
    "     '../Satellite/test/191_Alborg_2000_4000.png',\n",
    "     '../Satellite/test/265_Oostende_4000_4000.png',\n",
    "     '../Satellite/test/9_Rade De Brest_0_4000.png',\n",
    "     '../Satellite/test/18_Cork_2000_21000.png',\n",
    "     '../Satellite/test/356_Rostock_4000_8000.png',\n",
    "     '../Satellite/test/334_Fredericia_0_4000.png',\n",
    "     '../Satellite/test/359_Gdansk_11000_11000.png',\n",
    "     '../Satellite/test/317_Livorno_2000_5000.png',\n",
    "     '../Satellite/test/287_Gravesend_5000_3000.png',\n",
    "     '../Satellite/test/326_Porvoo_3000_1000.png',\n",
    "     '../Satellite/test/191_Alborg_1000_7000.png',\n",
    "     '../Satellite/test/349_Vlores_16000_7000.png',\n",
    "     '../Satellite/test/328_Tuapse_1000_5000.png',\n",
    "     '../Satellite/test/108_Parnu_2000_5000.png',\n",
    "     '../Satellite/test/186_Ghent_8000_16000.png',\n",
    "     '../Satellite/test/183_Antwerp_18000_15000.png',\n",
    "     '../Satellite/test/353_Ijmuiden_4000_1000.png',\n",
    "     '../Satellite/test/106_Hamina_0_3000.png',\n",
    "     '../Satellite/test/349_Vlores_15000_11000.png',\n",
    "     '../Satellite/test/72_Stenungsund_2000_3000.png',\n",
    "     '../Satellite/test/359_Gdansk_6000_3000.png',\n",
    "     '../Satellite/test/319_Brunsbuttel Elbahafen_0_2000.png',\n",
    "     '../Satellite/test/251_Cartagena_1000_7000.png',\n",
    "     '../Satellite/test/350_Kingston Upon Hull_3000_7000.png',\n",
    "     '../Satellite/test/319_Brunsbuttel Elbahafen_1000_0.png',\n",
    "     '../Satellite/test/229_Hamburg_9000_9000.png',\n",
    "     '../Satellite/test/209_Porto Di Lido-Venezia_7000_1000.png',\n",
    "     '../Satellite/test/359_Gdansk_13000_11000.png',\n",
    "     '../Satellite/test/361_Teesport_5000_2000.png',\n",
    "     '../Satellite/test/329_Cagliari_3000_4000.png',\n",
    "     '../Satellite/test/104_Lisboa_10000_20000.png',\n",
    "     '../Satellite/test/271_London_0_15000.png',\n",
    "     '../Satellite/test/86_Terneuzen_0_2000.png',\n",
    "     '../Satellite/test/183_Antwerp_2000_9000.png',\n",
    "     '../Satellite/test/293_Port of Le Havre_2000_13000.png',\n",
    "     '../Satellite/test/323_Fos_4000_0.png',\n",
    "     '../Satellite/test/107_Batumi_0_3000.png',\n",
    "     '../Satellite/test/349_Vlores_12000_15000.png',\n",
    "     '../Satellite/test/186_Ghent_11000_9000.png',\n",
    "     '../Satellite/test/159_Tarragona_3000_2000.png',\n",
    "     '../Satellite/test/271_London_1000_20000.png',\n",
    "     '../Satellite/test/190_Porto Torres_3000_0.png',\n",
    "     '../Satellite/test/120_Sarroch Oil Terminal_0_0.png',\n",
    "     '../Satellite/test/349_Vlores_15000_17000.png',\n",
    "     '../Satellite/test/105_Port-De-Bouc_2000_0.png',\n",
    "     '../Satellite/test/349_Vlores_15000_15000.png',\n",
    "     '../Satellite/test/335_Oil terminal Agioi_1000_0.png',\n",
    "     '../Satellite/test/191_Alborg_1000_5000.png',\n",
    "     '../Satellite/test/81_Porto Di Corsini_5000_2000.png',\n",
    "     '../Satellite/test/301_Rotterdam_18000_43000.png',\n",
    "     '../Satellite/test/349_Vlores_5000_9000.png',\n",
    "     '../Satellite/test/297_Ormos Aliveriou_1000_0.png',\n",
    "     '../Satellite/test/114_Milford Haven_7000_8000.png',\n",
    "     '../Satellite/test/280_Immingham_4000_0.png',\n",
    "     '../Satellite/test/271_London_2000_1000.png',\n",
    "]\n",
    "image_paths_overlap_2 = [\n",
    "     '../Satellite/test/293_Port of Le Havre_4000_24000.png',\n",
    "     '../Satellite/test/98_Constanta_7000_1000.png',\n",
    "     '../Satellite/test/301_Rotterdam_9000_21000.png',\n",
    "     '../Satellite/test/324_Elevsis_2000_0.png',\n",
    "     '../Satellite/test/349_Vlores_4000_8000.png',\n",
    "     '../Satellite/test/72_Stenungsund_2000_1000.png',\n",
    "     '../Satellite/test/372_Augusta_7000_3000.png',\n",
    "     '../Satellite/test/44_Derince Burnu_4000_8000.png',\n",
    "     '../Satellite/test/57_Milazzo_1000_1000.png',\n",
    "     '../Satellite/test/183_Antwerp_16000_17000.png',\n",
    "     '../Satellite/test/104_Lisboa_12000_20000.png',\n",
    "     '../Satellite/test/349_Vlores_15000_6000.png',\n",
    "     '../Satellite/test/349_Vlores_6000_12000.png',\n",
    "     '../Satellite/test/287_Gravesend_4000_2000.png',\n",
    "     '../Satellite/test/326_Porvoo_0_2000.png',\n",
    "     '../Satellite/test/284_Gela_0_1000.png',\n",
    "     '../Satellite/test/361_Teesport_6000_7000.png',\n",
    "     '../Satellite/test/285_Goole_1000_4000.png',\n",
    "     '../Satellite/test/349_Vlores_16000_15000.png',\n",
    "     '../Satellite/test/273_Sevilla_14000_1000.png',\n",
    "     '../Satellite/test/229_Hamburg_5000_17000.png',\n",
    "     '../Satellite/test/287_Gravesend_4000_0.png',\n",
    "     '../Satellite/test/365_Taranto_5000_6000.png',\n",
    "     '../Satellite/test/370_Puerto De Bilbao_6000_11000.png',\n",
    "     '../Satellite/test/285_Goole_0_4000.png',\n",
    "     '../Satellite/test/105_Port-De-Bouc_3000_3000.png',\n",
    "     '../Satellite/test/151_Vlissingen_5000_10000.png',\n",
    "     '../Satellite/test/229_Hamburg_4000_17000.png',\n",
    "     '../Satellite/test/60_Port Saint Louis Du Rhone_2000_3000.png',\n",
    "     '../Satellite/test/18_Cork_19000_26000.png',\n",
    "     '../Satellite/test/271_London_10000_32000.png',\n",
    "     '../Satellite/test/69_Kavala_0_1000.png',\n",
    "     '../Satellite/test/301_Rotterdam_9000_14000.png',\n",
    "     '../Satellite/test/86_Terneuzen_0_4000.png',\n",
    "     '../Satellite/test/349_Vlores_3000_9000.png',\n",
    "]\n",
    "\n",
    "image_paths_truth_1_pred_0 = [\n",
    "    '../Satellite/test/307_Aliaga_0_3000.png',\n",
    "]\n",
    "\n",
    "image_paths_truth_0_pred_1 = [\n",
    "    '../Satellite/test/26_Mongstad_3000_2000.png',\n",
    "     '../Satellite/test/36_Wismar_1000_3000.png',\n",
    "     '../Satellite/test/190_Porto Torres_4000_5000.png',\n",
    "     '../Satellite/test/60_Port Saint Louis Du Rhone_1000_2000.png',\n",
    "     '../Satellite/test/82_Dunkerque Port Ouest_4000_16000.png',\n",
    "     '../Satellite/test/229_Hamburg_10000_9000.png',\n",
    "     '../Satellite/test/183_Antwerp_16000_13000.png',\n",
    "     '../Satellite/test/159_Tarragona_1000_2000.png',\n",
    "     '../Satellite/test/106_Hamina_7000_1000.png',\n",
    "     '../Satellite/test/120_Sarroch Oil Terminal_2000_1000.png',\n",
    "     '../Satellite/test/301_Rotterdam_8000_16000.png',\n",
    "     '../Satellite/test/275_Tilbury_5000_3000.png',\n",
    "     '../Satellite/test/241_Bremen_3000_4000.png',\n",
    "     '../Satellite/test/108_Parnu_1000_4000.png',\n",
    "     '../Satellite/test/301_Rotterdam_19000_45000.png',\n",
    "     '../Satellite/test/209_Porto Di Lido-Venezia_7000_3000.png',\n",
    "]\n",
    "\n",
    "annotation_path = '../Satellite/test/via_region_data_with_empty_annotations.json'\n",
    "\n",
    "# Plot images\n",
    "def plot_images_by_category(image_paths, annotation_path, df_eval, category_title):\n",
    "    n_images = len(image_paths)\n",
    "    fig, axs = plt.subplots(n_images, 2, figsize=(20, 10 * n_images))\n",
    "    if n_images == 1:\n",
    "        axs = [axs]  # Ensure axs is iterable if there's only one image\n",
    "\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        plot_single_image(image_path, annotation_path, df_eval, axs[i][0], f\"{category_title} - Ground Truth\", is_prediction=False)\n",
    "        plot_single_image(image_path, annotation_path, df_eval, axs[i][1], f\"{category_title} - Predictions\", is_prediction=True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot each category\n",
    "#plot_images_by_category(image_paths_overlap_1, annotation_path, df_eval, 'True Positive')\n",
    "plot_images_by_category(image_paths_overlap_2, annotation_path, df_eval, 'True Positive')\n",
    "#plot_images_by_category(image_paths_truth_1_pred_0, annotation_path, df_eval, 'False Negative')\n",
    "#plot_images_by_category(image_paths_truth_0_pred_1, annotation_path, df_eval, 'False Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confusion matrix all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the confusion matrix values based on the user's correction\n",
    "\n",
    "data_corrected_v2 = {\n",
    "    \"Empty\": [14, 7, 12, 15, 1, 8, 0],\n",
    "    \"Container\": [1, 2, 1, 2, 1, 24, 13],\n",
    "    \"Refinery\": [2, 0, 4, 7, 69, 1, 35],  # Corrected values here\n",
    "    \"Oil & Gas\": [3, 1, 4, 30, 7, 4, 20],\n",
    "    \"Raw\": [0, 0, 21, 1, 1, 2, 29],\n",
    "    \"RoRo\": [1, 9, 1, 0, 0, 2, 7],\n",
    "    \"Warehouse\": [15, 1, 1, 0, 0, 2, 19]\n",
    "}\n",
    "index_corrected_v2 = [\"Warehouse\", \"RoRo\", \"Raw\", \"Oil & Gas\", \"Refinery\", \"Container\", \"Empty\"]\n",
    "df_corrected_v2 = pd.DataFrame(data_corrected_v2, index=index_corrected_v2)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_corrected_v2, annot=True, cmap=\"Reds\", fmt=\"d\", linewidths=.5)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Corrected confusion matrix data\n",
    "data_corrected_v2 = {\n",
    "    \"Empty\": [14, 7, 12, 15, 1, 8, 0],\n",
    "    \"Container\": [1, 2, 1, 2, 1, 24, 13],\n",
    "    \"Refinery\": [2, 0, 4, 7, 69, 1, 35],\n",
    "    \"Oil & Gas\": [3, 1, 4, 30, 7, 4, 20],\n",
    "    \"Raw\": [0, 0, 21, 1, 1, 2, 29],\n",
    "    \"RoRo\": [1, 9, 1, 0, 0, 2, 7],\n",
    "    \"Warehouse\": [15, 1, 1, 0, 0, 2, 19]\n",
    "}\n",
    "index_corrected_v2 = [\"Warehouse\", \"RoRo\", \"Raw\", \"Oil & Gas\", \"Refinery\", \"Container\", \"Empty\"]\n",
    "\n",
    "categories = [\"Warehouse\", \"RoRo\", \"Raw\", \"Oil & Gas\", \"Refinery\", \"Container\", \"Empty\"]\n",
    "\n",
    "# Calculating TP, TN, FP, FN\n",
    "TP_TN_FP_FN = {}\n",
    "\n",
    "# Total number of instances\n",
    "N = 4305\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    TP = data_corrected_v2[category][i]\n",
    "    FP = sum(data_corrected_v2[category]) - TP\n",
    "    FN = sum([data_corrected_v2[label][i] for label in categories]) - TP\n",
    "    TN = N - (TP + FP + FN)\n",
    "    TP_TN_FP_FN[category] = {\"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN}\n",
    "\n",
    "# Displaying the results\n",
    "tp_tn_fp_fn_df = pd.DataFrame(TP_TN_FP_FN).T\n",
    "print(tp_tn_fp_fn_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Define the confusion matrix data for each class using the provided numbers\n",
    "confusion_data_updated = {\n",
    "    \"Warehouse\": {\"TP\": 15, \"TN\": 0, \"FP\": 18, \"FN\": 14},\n",
    "    \"RoRo\": {\"TP\": 9, \"TN\": 0, \"FP\": 12, \"FN\": 7},\n",
    "    \"Raw\": {\"TP\": 21, \"TN\": 0, \"FP\": 33, \"FN\": 12},\n",
    "    \"Oil & Gas\": {\"TP\": 30, \"TN\": 0, \"FP\": 39, \"FN\": 15},\n",
    "    \"Refinery\": {\"TP\": 69, \"TN\": 0, \"FP\": 49, \"FN\": 1},\n",
    "    \"Container\": {\"TP\": 24, \"TN\": 0, \"FP\": 20, \"FN\": 7}\n",
    "}\n",
    "\n",
    "# Function to create a 2x2 confusion matrix heatmap\n",
    "def plot_confusion_matrix(data, ax, title):\n",
    "    matrix = np.array([[data[\"TP\"], data[\"FN\"]],\n",
    "                       [data[\"FP\"], data[\"TN\"]]])\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.xaxis.set_ticklabels(['Positive', 'Negative'])\n",
    "    ax.yaxis.set_ticklabels(['Positive', 'Negative'])\n",
    "\n",
    "# Create subplots for each confusion matrix\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot each confusion matrix\n",
    "plot_confusion_matrix(confusion_data_updated[\"Warehouse\"], axes[0, 0], \"Warehouse\")\n",
    "plot_confusion_matrix(confusion_data_updated[\"RoRo\"], axes[0, 1], \"RoRo\")\n",
    "plot_confusion_matrix(confusion_data_updated[\"Raw\"], axes[0, 2], \"Raw\")\n",
    "plot_confusion_matrix(confusion_data_updated[\"Oil & Gas\"], axes[1, 0], \"Oil & Gas\")\n",
    "plot_confusion_matrix(confusion_data_updated[\"Refinery\"], axes[1, 1], \"Refinery\")\n",
    "plot_confusion_matrix(confusion_data_updated[\"Container\"], axes[1, 2], \"Container\")\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
