{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor, hooks\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset, print_csv_format\n",
    "\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import detectron2\n",
    "import pandas as pd\n",
    "\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from train import get_dataset_dicts, get_dataset_dicts_with_regions, random_visu, setup_cfg, MyTrainer, load_json_arr, find_best_model\n",
    "\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test directory is created!\n"
     ]
    }
   ],
   "source": [
    "### FOLDER PATH ###\n",
    "\n",
    "dataset_eval = \"test\"\n",
    "dataset_eval_output = \"val_output_raw_data\"\n",
    "annotation_json = \"via_region_data_raw_annotations.json\"\n",
    "results_file = \"results.json\"\n",
    "experiment = \"../Satellite/output/lr001_BS4_raw_test__aug_1\"\n",
    "conf_path = \"../Satellite/output/lr001_BS4_raw_test__aug_1/NSOD2cfg_1000_169r_1000pix_noBT_lr001_BS4_raw_test__aug_1.yaml\"\n",
    "model_path = experiment + \"/model_37.pth\"\n",
    "filter_empty_annot = False\n",
    "\n",
    "\n",
    "annotation_output = os.path.splitext(annotation_json)[0]\n",
    "out_dir = os.path.join(experiment, dataset_eval_output, annotation_output)\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    print (\"The test directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dicts(img_dir, annotation_json):\n",
    "    with open(os.path.join(img_dir, annotation_json)) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(annotations.values()):\n",
    "        record = {}\n",
    "        filename = os.path.join(img_dir, v[\"filename\"])\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        \n",
    "        annos = v[\"regions\"]\n",
    "        objs = []\n",
    "        for anno_key, anno in annos.items():\n",
    "            shape_attr = anno[\"shape_attributes\"]\n",
    "            region_attr = anno[\"region_attributes\"]\n",
    "            \n",
    "            # Ensure that region_attributes is processed correctly\n",
    "            if \"type\" not in region_attr or region_attr[\"type\"] != \"raw\":\n",
    "                continue  # Skip non-container regions if any\n",
    "\n",
    "            px = shape_attr[\"all_points_x\"]\n",
    "            py = shape_attr[\"all_points_y\"]\n",
    "            poly = [(x, y) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0,  # Assuming 'container' is the only category\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Indicate to D2 how to read dataset if not in coco format: ###\n",
    "\n",
    "# D2 metadata: name of classes and colors of annotations\n",
    "classes = [\"raw\"]\n",
    "colors = [(249, 180, 45)]\n",
    "\n",
    "# Register dataset and metadata\n",
    "for d in [\"train\", \"val\", \"test\"]:\n",
    "    DatasetCatalog.register(d, lambda d=d:\n",
    "                            get_dataset_dicts(os.path.join(\"../Satellite\", d), annotation_json))\n",
    "    # Key-value mapping to interpret whatâ€™s in the dataset: names of classes, colors of classes\n",
    "    MetadataCatalog.get(d).thing_classes = classes\n",
    "    MetadataCatalog.get(d).thing_colors = colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config ../Satellite/output/lr001_BS4_raw_test_13/NSOD2cfg_1000_169r_1000pix_noBT_lr001_BS4_raw_test_13.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
      "/scistor/ivm/fjo101/miniconda3/envs/nso/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/scistor/ivm/fjo101/miniconda3/envs/nso/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.911\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.970\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.656\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.893\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.938\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.803\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.933\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.933\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.662\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.912\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.956\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.891\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.979\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.639\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.879\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.917\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.788\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.912\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.912\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.892\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.935\n",
      "Results dumped\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "# keep those 2 to avoid errors of MIN_SIZE_TRAIN = 800 and resize not found\n",
    "#change in config: \n",
    "#MIN_SIZE_TRAIN: !!python/tuple\n",
    "#- 1000\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (1000,)\n",
    "cfg.RESIZE= False\n",
    "cfg.merge_from_file(conf_path)\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (1000,)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = model_path\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = filter_empty_annot\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "evaluator = COCOEvaluator(dataset_eval, output_dir=out_dir)\n",
    "\n",
    "val_loader = build_detection_test_loader(cfg, dataset_eval)\n",
    "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "\n",
    "with open(os.path.join(out_dir, results_file), 'w') as f:\n",
    "    json.dump(results, f)\n",
    "print(\"Results dumped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_dir = '../NSO/output/lr001_BS4_empty-annot-50/eval'\n",
    "# Load the JSON pred file into a DataFrame\n",
    "df_pred = pd.read_json(os.path.join(out_dir, 'coco_instances_results.json'))\n",
    "df_pred = df_pred[df_pred['score'] > 0.5] \n",
    "\n",
    "# Load the JSON ground truth file into a DataFrame\n",
    "\n",
    "# CHANGE VAL OR TEST\n",
    "# Extract the \"images\" key and create a DataFrame from it\n",
    "\n",
    "# Ground thruth OG\n",
    "df_truth_og = pd.read_json(os.path.join(os.path.join(\"../Satellite\", dataset_eval), annotation_json))\n",
    "with open(os.path.join(out_dir, 'test_coco_format.json'), 'r') as j:\n",
    "     data = json.loads(j.read())\n",
    "images_data = data.get('images', [])  # Get the list of \"images\" objects\n",
    "annotation_data = data.get('annotations', [])  # Get the list of \"categories\" objects\n",
    "# Create a DataFrame from the valid \"images\" objects\n",
    "df_images = pd.DataFrame(images_data)\n",
    "df_annotations = pd.DataFrame(annotation_data)\n",
    "#Drop duplicates: where are they comming from???? =>val_coco is automatically done by D2 check if where nitially in dataset\n",
    "df_annotations = df_annotations.drop_duplicates(subset=['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth = pd.merge(df_images, df_annotations, left_on='id', right_on='image_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop duplicates => why are they there?\n",
    "#df_truth = df_truth.drop_duplicates(subset=['bbox'])\n",
    "df_eval = pd.merge(df_truth, df_pred, left_on='id_x', right_on='image_id', how='left')\n",
    "# df_eval = df_eval.drop_duplicates(subset=['segmentation_x'])\n",
    "# #draop images with several annotation, just need to keep one\n",
    "df_eval = df_eval.drop_duplicates(subset=['id_x'])\n",
    "df_eval['truth'] = None\n",
    "df_eval['pred'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['truth'] = np.where(df_eval.bbox_x.isnull(), 0, 1)\n",
    "df_eval['pred'] = np.where(df_eval.score.isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#ravel to flatten conf matrix to 1D insted of 2D\n",
    "tn, fp, fn, tp = confusion_matrix(df_eval['truth'], df_eval['pred']).ravel()\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "print(\"Precision: \", precision_score)\n",
    "print(\"Recall: \", recall_score)\n",
    "print (\"Fscore: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert int64 values to native Python int\n",
    "tn = int(tn)\n",
    "fp = int(fp)\n",
    "fn = int(fn)\n",
    "tp = int(tp)\n",
    "\n",
    "metrics_dict = {\n",
    "    'tn': tn,\n",
    "    'fp': fp,\n",
    "    'fn': fn,\n",
    "    'tp': tp,\n",
    "    'precision_score': precision_score,\n",
    "    'recall_score': recall_score,\n",
    "    'f1_score': f1_score\n",
    "}\n",
    "\n",
    "# Define the file path\n",
    "json_metrics = 'metrics_tiles.json'\n",
    "json_path = os.path.join(out_dir, json_metrics)\n",
    "\n",
    "# Save the metrics to a JSON file\n",
    "with open(json_path, 'w') as json_file:\n",
    "    json.dump(metrics_dict, json_file, indent=4)\n",
    "\n",
    "print(f'Metrics saved to {json_file}')\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(df_eval['truth'], df_eval['pred'])\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix with enhanced visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.4)  # Adjust to increase the size of the labels\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Predicted: Not Raw Material', 'Predicted: Raw Material'],\n",
    "            yticklabels=['Actual: Not Raw Material', 'Actual: Raw Material'])\n",
    "plt.xlabel('Prediction', fontsize=16)\n",
    "plt.ylabel('Truth', fontsize=16)\n",
    "plt.title('Confusion Matrix', fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "import random\n",
    "\n",
    "# Function to load image\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "    return image\n",
    "\n",
    "# Function to visualize ground truth and predictions\n",
    "def visualize_predictions(image_path, ground_truth, predictions):\n",
    "    image = load_image(image_path)\n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    visualizer = Visualizer(image[:, :, ::-1], MetadataCatalog.get(dataset_eval), scale=1.2)\n",
    "    \n",
    "    # Draw ground truth\n",
    "    if ground_truth is not None:\n",
    "        for gt in ground_truth:\n",
    "            bbox = gt['bbox_x']\n",
    "            # Check if bbox is correctly formatted\n",
    "            if len(bbox) != 4:\n",
    "                print(f\"Invalid Ground Truth BBox format: {bbox}\")\n",
    "                continue\n",
    "            bbox_mode = BoxMode.convert(bbox, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)  # Assuming bbox is in XYWH format\n",
    "            print(f\"Ground Truth BBox: {bbox_mode}\")\n",
    "            visualizer.draw_box(bbox_mode, edge_color=(0, 1, 0, 1))  # RGBA values within 0-1 range\n",
    "            visualizer.draw_text(\"GT\", bbox_mode[:2], color='g', font_size=10)\n",
    "    \n",
    "    # Draw predictions\n",
    "    if predictions is not None:\n",
    "        for pred in predictions:\n",
    "            bbox = pred['bbox_y']\n",
    "            # Check if bbox is correctly formatted\n",
    "            if len(bbox) != 4:\n",
    "                print(f\"Invalid Prediction BBox format: {bbox}\")\n",
    "                continue\n",
    "            bbox_mode = BoxMode.convert(bbox, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)  # Assuming bbox is already in XYXY format\n",
    "            score = pred['score']\n",
    "            print(f\"Prediction BBox: {bbox_mode}, Score: {score}\")\n",
    "            visualizer.draw_box(bbox_mode, edge_color=(1, 0, 0, 1))  # RGBA values within 0-1 range\n",
    "            visualizer.draw_text(f\"Pred: {score:.2f}\", bbox_mode[:2], color='r', font_size=10)\n",
    "    \n",
    "    vis_image = visualizer.get_output().get_image()[:, :, ::-1]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(vis_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ensure dataset_eval is defined\n",
    "dataset_eval = \"test\"  # Change this to the appropriate dataset name if different\n",
    "\n",
    "# Visualize 10 random examples\n",
    "image_dir = \"../Satellite/test\"  # Correct base path for the test images\n",
    "unique_image_names = df_eval['file_name'].unique()\n",
    "random_images = random.sample(list(unique_image_names), 10)\n",
    "\n",
    "for full_image_name in random_images:\n",
    "    # Get image file path\n",
    "    image_path = full_image_name\n",
    "    \n",
    "    # Print the file path to ensure correctness\n",
    "    print(f\"Image Path: {image_path}\")\n",
    "    \n",
    "    # Get ground truth and predictions for the specific image using the full path\n",
    "    gt_info = df_eval[df_eval['file_name'] == full_image_name]\n",
    "    print(f\"Ground truth records found: {len(gt_info)}\")\n",
    "    \n",
    "    ground_truth = gt_info[~gt_info['bbox_x'].isnull()][['bbox_x', 'category_id_x']].to_dict('records')\n",
    "    \n",
    "    # Print ground truth information for debugging\n",
    "    print(\"Ground Truth Info:\")\n",
    "    for gt in ground_truth:\n",
    "        print(gt)\n",
    "    \n",
    "    pred_info = df_eval[df_eval['file_name'] == full_image_name]\n",
    "    print(f\"Prediction records found: {len(pred_info)}\")\n",
    "    \n",
    "    predictions = pred_info[~pred_info['bbox_y'].isnull()][['bbox_y', 'score', 'category_id_y']].to_dict('records')\n",
    "    \n",
    "    # Print prediction information for debugging\n",
    "    print(\"Prediction Info:\")\n",
    "    for pred in predictions:\n",
    "        print(pred)\n",
    "    \n",
    "    # Visualize predictions on the image\n",
    "    visualize_predictions(image_path, ground_truth, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "import random\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "# Function to load image\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "    return image\n",
    "\n",
    "# Function to convert polygon segmentation to binary mask\n",
    "def polygon_to_mask(segmentation, height, width):\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    points = np.array(segmentation).reshape(-1, 2)\n",
    "    cv2.fillPoly(mask, [points], 1)\n",
    "    return mask\n",
    "\n",
    "# Function to visualize ground truth and predictions with segmentation masks\n",
    "def visualize_predictions(image_path, ground_truth, predictions):\n",
    "    image = load_image(image_path)\n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    visualizer = Visualizer(image[:, :, ::-1], MetadataCatalog.get(dataset_eval), scale=1.2)\n",
    "    \n",
    "    # Draw ground truth masks\n",
    "    if ground_truth is not None:\n",
    "        for gt in ground_truth:\n",
    "            segmentation = gt['segmentation_x']\n",
    "            print(f\"Ground Truth Segmentation: {segmentation}\")\n",
    "            mask = polygon_to_mask(segmentation, image.shape[0], image.shape[1])\n",
    "            visualizer.draw_binary_mask(mask, color=(0, 1, 0, 0.3), edge_color=(0, 1, 0, 1))\n",
    "            visualizer.draw_text(\"GT\", (segmentation[0][0], segmentation[0][1]), color='g', font_size=10)\n",
    "    \n",
    "    # Draw prediction masks\n",
    "    if predictions is not None:\n",
    "        for pred in predictions:\n",
    "            segmentation = pred['segmentation_y']\n",
    "            score = pred['score']\n",
    "            print(f\"Prediction Segmentation: {segmentation}, Score: {score}\")\n",
    "            mask = mask_util.decode(segmentation)\n",
    "            visualizer.draw_binary_mask(mask, color=(1, 0, 0, 0.3), edge_color=(1, 0, 0, 1))\n",
    "            visualizer.draw_text(f\"Pred: {score:.2f}\", (mask.shape[1] // 2, mask.shape[0] // 2), color='r', font_size=10)\n",
    "    \n",
    "    vis_image = visualizer.get_output().get_image()[:, :, ::-1]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(vis_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ensure dataset_eval is defined\n",
    "dataset_eval = \"test\"  # Change this to the appropriate dataset name if different\n",
    "\n",
    "# Visualize 10 random examples\n",
    "image_dir = \"../Satellite/test\"  # Correct base path for the test images\n",
    "unique_image_names = df_eval['file_name'].unique()\n",
    "random_images = random.sample(list(unique_image_names), 10)\n",
    "\n",
    "for full_image_name in random_images:\n",
    "    # Get image file path\n",
    "    image_path = full_image_name\n",
    "    \n",
    "    # Print the file path to ensure correctness\n",
    "    print(f\"Image Path: {image_path}\")\n",
    "    \n",
    "    # Get ground truth and predictions for the specific image using the full path\n",
    "    gt_info = df_eval[df_eval['file_name'] == full_image_name]\n",
    "    print(f\"Ground truth records found: {len(gt_info)}\")\n",
    "    \n",
    "    ground_truth = gt_info[~gt_info['segmentation_x'].isnull()][['segmentation_x', 'category_id_x']].to_dict('records')\n",
    "    \n",
    "    # Print ground truth information for debugging\n",
    "    print(\"Ground Truth Info:\")\n",
    "    for gt in ground_truth:\n",
    "        print(gt)\n",
    "    \n",
    "    pred_info = df_eval[df_eval['file_name'] == full_image_name]\n",
    "    print(f\"Prediction records found: {len(pred_info)}\")\n",
    "    \n",
    "    predictions = pred_info[~pred_info['segmentation_y'].isnull()][['segmentation_y', 'score', 'category_id_y']].to_dict('records')\n",
    "    \n",
    "    # Print prediction information for debugging\n",
    "    print(\"Prediction Info:\")\n",
    "    for pred in predictions:\n",
    "        print(pred)\n",
    "    \n",
    "    # Visualize predictions on the image\n",
    "    visualize_predictions(image_path, ground_truth, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for overlaps where truth and pred are both 1\n",
    "overlaps_truth_pred_1 = df_eval[(df_eval['truth'] == 1) & (df_eval['pred'] == 1)]\n",
    "\n",
    "# Filter for non-overlaps where truth is 1 and pred is 0\n",
    "non_overlaps_truth_1_pred_0 = df_eval[(df_eval['truth'] == 1) & (df_eval['pred'] == 0)]\n",
    "\n",
    "# Filter for truth == 0 and pred == 1\n",
    "truth_0_pred_1 = df_eval[(df_eval['truth'] == 0) & (df_eval['pred'] == 1)]\n",
    "\n",
    "# Filter for truth == 0 and pred == 0\n",
    "truth_0_pred_0 = df_eval[(df_eval['truth'] == 0) & (df_eval['pred'] == 0)]\n",
    "\n",
    "# Extract unique image IDs for overlaps (truth == 1 and pred == 1)\n",
    "overlap_image_ids = overlaps_truth_pred_1['file_name'].unique()\n",
    "\n",
    "# Extract unique image IDs for non-overlaps (truth == 1 and pred == 0)\n",
    "non_overlap_image_ids = non_overlaps_truth_1_pred_0['file_name'].unique()\n",
    "\n",
    "# Extract unique image IDs for truth == 0 and pred == 1\n",
    "truth_0_pred_1_image_ids = truth_0_pred_1['file_name'].unique()\n",
    "\n",
    "# Extract unique image IDs for truth == 0 and pred == 0\n",
    "truth_0_pred_0_image_ids = truth_0_pred_0['file_name'].unique()\n",
    "\n",
    "# Print the results with counts\n",
    "print(\"Image IDs where truth and prediction overlap (truth == 1 and pred == 1):\")\n",
    "print(overlap_image_ids)\n",
    "print(f\"Count: {len(overlap_image_ids)}\\n\")\n",
    "\n",
    "print(\"Image IDs where truth is 1 and prediction is 0:\")\n",
    "print(non_overlap_image_ids)\n",
    "print(f\"Count: {len(non_overlap_image_ids)}\\n\")\n",
    "\n",
    "print(\"Image IDs where truth is 0 and prediction is 1:\")\n",
    "print(truth_0_pred_1_image_ids)\n",
    "print(f\"Count: {len(truth_0_pred_1_image_ids)}\\n\")\n",
    "\n",
    "print(\"Image IDs where truth is 0 and prediction is 0:\")\n",
    "print(truth_0_pred_0_image_ids)\n",
    "print(f\"Count: {len(truth_0_pred_0_image_ids)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util\n",
    "import cv2\n",
    "\n",
    "# Function to convert polygon segmentation to binary mask\n",
    "def polygon_to_mask(segmentation, height, width):\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    points = np.array(segmentation).reshape(-1, 2)\n",
    "    cv2.fillPoly(mask, [points], 1)\n",
    "    return mask\n",
    "\n",
    "# Function to plot a single image with annotations\n",
    "def plot_single_image(tile_path, annotation_path, df_eval, ax, title, is_prediction=False):\n",
    "    # Load the image\n",
    "    image = Image.open(tile_path)\n",
    "    \n",
    "    # Load the annotation JSON file\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Extract the filename from the tile path\n",
    "    filename = tile_path.split('/')[-1]\n",
    "    \n",
    "    # Get the annotations for the specified tile\n",
    "    tile_annotations = annotations.get(filename, {}).get('regions', {})\n",
    "    \n",
    "    # Get predictions from df_eval\n",
    "    predictions = df_eval[df_eval['file_name'] == tile_path].to_dict('records')\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Function to plot regions\n",
    "    def plot_regions(ax, regions, edgecolor, facecolor, label_suffix):\n",
    "        for region in regions.values():\n",
    "            shape_attributes = region.get('shape_attributes', {})\n",
    "            region_attributes = region.get('region_attributes', {})\n",
    "            \n",
    "            if shape_attributes.get('name') == 'polygon':\n",
    "                all_points_x = shape_attributes.get('all_points_x', [])\n",
    "                all_points_y = shape_attributes.get('all_points_y', [])\n",
    "                polygon = list(zip(all_points_x, all_points_y))\n",
    "                \n",
    "                # Determine color based on type\n",
    "                annotation_type = region_attributes.get('type', 'unknown')\n",
    "                if annotation_type == 'raw':\n",
    "                    edgecolor = 'g'\n",
    "                    facecolor = 'green'\n",
    "                else:\n",
    "                    edgecolor = 'r'\n",
    "                    facecolor = 'red'\n",
    "                \n",
    "                # Create a polygon patch with a semi-transparent fill color\n",
    "                poly_patch = patches.Polygon(polygon, closed=True, edgecolor=edgecolor, facecolor=facecolor, alpha=0.5)\n",
    "                ax.add_patch(poly_patch)\n",
    "                \n",
    "                # Get the centroid of the polygon to place the label\n",
    "                centroid_x = sum(all_points_x) / len(all_points_x)\n",
    "                centroid_y = sum(all_points_y) / len(all_points_y)\n",
    "                \n",
    "                # Get the type/category for the annotation\n",
    "                annotation_type = region_attributes.get('type', 'unknown')\n",
    "                \n",
    "                # Add text annotation to the plot\n",
    "                ax.text(centroid_x, centroid_y, f\"{annotation_type} {label_suffix}\", color='white', fontsize=12, ha='center', va='center',\n",
    "                        bbox=dict(facecolor=edgecolor, alpha=0.5, edgecolor='none', pad=1))\n",
    "    \n",
    "    # Plot ground truth annotations\n",
    "    if not is_prediction:\n",
    "        plot_regions(ax, tile_annotations, edgecolor='r', facecolor='red', label_suffix='(GT)')\n",
    "    \n",
    "    # Plot predictions from df_eval\n",
    "    if is_prediction:\n",
    "        for pred in predictions:\n",
    "            if 'segmentation_y' in pred and pred['segmentation_y']:\n",
    "                segmentation = pred['segmentation_y']\n",
    "                score = pred['score']\n",
    "                if isinstance(segmentation, dict) and 'size' in segmentation and 'counts' in segmentation:  # Check if it's a RLE\n",
    "                    mask = mask_util.decode(segmentation)\n",
    "                    poly_patch = patches.Polygon(np.argwhere(mask).reshape(-1, 2), closed=True, edgecolor='b', facecolor='blue', alpha=0.5)\n",
    "                    ax.add_patch(poly_patch)\n",
    "                    centroid_x, centroid_y = np.mean(np.argwhere(mask), axis=0)\n",
    "                else:\n",
    "                    # Handle the polygon segmentation case\n",
    "                    if isinstance(segmentation, list):\n",
    "                        segmentation = np.array(segmentation).reshape(-1, 2)\n",
    "                        poly_patch = patches.Polygon(segmentation, closed=True, edgecolor='b', facecolor='blue', alpha=0.5)\n",
    "                        ax.add_patch(poly_patch)\n",
    "                        centroid_x = np.mean(segmentation[:, 0])\n",
    "                        centroid_y = np.mean(segmentation[:, 1])\n",
    "                    else:\n",
    "                        continue\n",
    "                # Add text annotation to the plot\n",
    "                ax.text(centroid_x, centroid_y, f\"Pred: {score:.2f}\", color='white', fontsize=12, ha='center', va='center',\n",
    "                        bbox=dict(facecolor='blue', alpha=0.5, edgecolor='none', pad=1))\n",
    "\n",
    "# Example data\n",
    "image_paths_overlap = [\n",
    "    '../Satellite/test/349_Vlores_16000_5000.png',\n",
    "     '../Satellite/test/301_Rotterdam_9000_9000.png',\n",
    "     '../Satellite/test/349_Vlores_16000_7000.png',\n",
    "     '../Satellite/test/353_Ijmuiden_4000_1000.png',\n",
    "     '../Satellite/test/253_Skelleftehamn_0_2000.png',\n",
    "     '../Satellite/test/90_Swinoujscie_0_5000.png',\n",
    "     '../Satellite/test/292_Liverpool_1000_0.png',\n",
    "     '../Satellite/test/28_Reka Luga_13000_15000.png',\n",
    "     '../Satellite/test/73_La Pallice_0_3000.png',\n",
    "     '../Satellite/test/89_Illichivsk_5000_5000.png',\n",
    "     '../Satellite/test/271_London_0_15000.png',\n",
    "     '../Satellite/test/211_Klaipeda_13000_7000.png',\n",
    "     '../Satellite/test/21_Inkoo_1000_1000.png',\n",
    "     '../Satellite/test/211_Klaipeda_15000_7000.png',\n",
    "     '../Satellite/test/100_Mariupol_1000_1000.png',\n",
    "     '../Satellite/test/75_Sines_7000_4000.png',\n",
    "     '../Satellite/test/125_Bayonne_6000_3000.png',\n",
    "     '../Satellite/test/98_Constanta_1000_3000.png',\n",
    "     '../Satellite/test/244_Eemshaven_3000_4000.png',\n",
    "     '../Satellite/test/244_Eemshaven_3000_9000.png',\n",
    "     '../Satellite/test/186_Ghent_22000_7000.png',\n",
    "     '../Satellite/test/349_Vlores_5000_14000.png',\n",
    "     '../Satellite/test/117_Barry_1000_1000.png',\n",
    "     '../Satellite/test/258_Lulea_4000_9000.png',\n",
    "     '../Satellite/test/304_Vostochnyy_1000_9000.png',\n",
    "     '../Satellite/test/308_Dublin_2000_2000.png',\n",
    "     '../Satellite/test/176_Riga_4000_5000.png',\n",
    "     '../Satellite/test/151_Vlissingen_4000_10000.png',\n",
    "     '../Satellite/test/265_Oostende_1000_1000.png',\n",
    "     '../Satellite/test/247_Murmansk_22000_7000.png',\n",
    "     '../Satellite/test/316_Durres_0_1000.png',\n",
    "     '../Satellite/test/24_Gijon_3000_4000.png',\n",
    "     '../Satellite/test/175_Dudinka_11000_6000.png',\n",
    "     '../Satellite/test/301_Rotterdam_8000_8000.png',\n",
    "     '../Satellite/test/216_Odense_1000_18000.png',\n",
    "     '../Satellite/test/271_London_5000_2000.png',\n",
    "     '../Satellite/test/349_Vlores_15000_6000.png',\n",
    "     '../Satellite/test/10_Huelva_6000_2000.png',\n",
    "     '../Satellite/test/202_Leith_1000_0.png',\n",
    "     '../Satellite/test/267_Sete_4000_2000.png',\n",
    "     '../Satellite/test/234_Shoreham Harbour_0_2000.png',\n",
    "     '../Satellite/test/244_Eemshaven_4000_7000.png',\n",
    "     '../Satellite/test/234_Shoreham Harbour_0_0.png',\n",
    "     '../Satellite/test/254_Nemrut Limani Bay_1000_2000.png',\n",
    "     '../Satellite/test/82_Dunkerque Port Ouest_1000_25000.png',\n",
    "     '../Satellite/test/371_Mykolayiv_0_1000.png',\n",
    "     '../Satellite/test/373_Norrkoping_4000_4000.png',\n",
    "     '../Satellite/test/151_Vlissingen_5000_10000.png',\n",
    "     '../Satellite/test/350_Kingston Upon Hull_1000_5000.png',\n",
    "     '../Satellite/test/28_Reka Luga_14000_15000.png',\n",
    "     '../Satellite/test/66_Belfast_5000_1000.png',\n",
    "     '../Satellite/test/210_Gdynia_3000_8000.png',\n",
    "     '../Satellite/test/98_Constanta_4000_2000.png',\n",
    "     '../Satellite/test/169_Randers_0_1000.png',\n",
    "]\n",
    "\n",
    "image_paths_truth_1_pred_0 = [\n",
    "    '../Satellite/test/359_Gdansk_3000_4000.png',\n",
    "     '../Satellite/test/359_Gdansk_10000_5000.png',\n",
    "     '../Satellite/test/382_Wilhelmshaven_12000_8000.png',\n",
    "     '../Satellite/test/183_Antwerp_17000_5000.png',\n",
    "     '../Satellite/test/191_Alborg_1000_5000.png',\n",
    "     '../Satellite/test/211_Klaipeda_14000_6000.png',\n",
    "     '../Satellite/test/145_Kaliningrad_1000_2000.png',\n",
    "     '../Satellite/test/146_Talinn_3000_1000.png',\n",
    "     '../Satellite/test/183_Antwerp_7000_13000.png',\n",
    "     '../Satellite/test/203_Vikanes_0_0.png',\n",
    "     '../Satellite/test/226_Torina_6000_1000.png',\n",
    "     '../Satellite/test/117_Barry_0_0.png',\n",
    "     '../Satellite/test/313_Koper_1000_0.png',\n",
    "     '../Satellite/test/308_Dublin_1000_0.png',\n",
    "     '../Satellite/test/382_Wilhelmshaven_11000_9000.png',\n",
    "     '../Satellite/test/334_Fredericia_1000_0.png',\n",
    "     '../Satellite/test/271_London_4000_3000.png',\n",
    "     '../Satellite/test/211_Klaipeda_16000_9000.png',\n",
    "]\n",
    "\n",
    "image_paths_truth_0_pred_1 = [\n",
    "]\n",
    "\n",
    "annotation_path = '../Satellite/test/via_region_data_with_empty_annotations.json'\n",
    "\n",
    "# Plot images\n",
    "def plot_images_by_category(image_paths, annotation_path, df_eval, category_title):\n",
    "    n_images = len(image_paths)\n",
    "    fig, axs = plt.subplots(n_images, 2, figsize=(20, 10 * n_images))\n",
    "    if n_images == 1:\n",
    "        axs = [axs]  # Ensure axs is iterable if there's only one image\n",
    "\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        plot_single_image(image_path, annotation_path, df_eval, axs[i][0], f\"{category_title} - Ground Truth\", is_prediction=False)\n",
    "        plot_single_image(image_path, annotation_path, df_eval, axs[i][1], f\"{category_title} - Predictions\", is_prediction=True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot each category\n",
    "plot_images_by_category(image_paths_overlap, annotation_path, df_eval, 'True Positive')\n",
    "plot_images_by_category(image_paths_truth_1_pred_0, annotation_path, df_eval, 'False Negative')\n",
    "plot_images_by_category(image_paths_truth_0_pred_1, annotation_path, df_eval, 'False Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
