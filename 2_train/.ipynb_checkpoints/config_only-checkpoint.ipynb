{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import detectron2\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "os.environ['LD_LIBRARY_PATH'] = '/cm/local/apps/gcc/11.2.0/lib:/cm/local/apps/gcc/11.2.0/lib64:/cm/local/apps/gcc/11.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64'\n",
    "\n",
    "\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/cm/local/apps/gcc/11.2.0/lib:/cm/local/apps/gcc/11.2.0/lib64:/cm/local/apps/gcc/11.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64'\n",
    "\n",
    "# os.environ['PATH'] ='/scistor/ivm/fjo101/miniconda3/envs/nso/bin:/scistor/ivm/fjo101/miniconda3/condabin:/scistor/ivm/fjo101/.local/bin:/scistor/ivm/fjo101/bin:/cm/local/apps/gcc/11.2.0/bin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU information\n",
    "print (torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.device(0)\n",
    "       , torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional info when using CUDA\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62daceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate to D2 how to read dataset if not in coco format:\n",
    "\n",
    "def get_dataset_dicts(img_dir):\n",
    "    \"\"\"Function that tells detectron2 know how to obtain a custom datasets by specifying it similarly to COCO’s annotations: see https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html \n",
    "\n",
    "    Args:\n",
    "        img_dir (str): path to the directory containing the images and annotations .json file \n",
    "\n",
    "    Returns:\n",
    "        list: list of dictionaries containing the information of the dataset\n",
    "    \"\"\"\n",
    "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
    "    with open(json_file) as f:\n",
    "        imgs_anns = json.load(f)\n",
    "    \n",
    "    dataset_dicts = []\n",
    "    for idx, annots in enumerate(imgs_anns.values()):\n",
    "        record = {}\n",
    "        \n",
    "        filename = os.path.join(img_dir, annots[\"filename\"])\n",
    "        height, width = cv.imread(filename).shape[:2]\n",
    "        \n",
    "        #Info on the tile .png file\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        \n",
    "        #info on annotation\n",
    "        annotations = annots[\"regions\"]\n",
    "        objs = []\n",
    "        for _, anno in annotations.items():\n",
    "            assert not anno[\"region_attributes\"]\n",
    "            anno = anno[\"shape_attributes\"]\n",
    "            px = anno[\"all_points_x\"]\n",
    "            py = anno[\"all_points_y\"]\n",
    "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px),\n",
    "                         np.min(py),\n",
    "                         np.max(px),\n",
    "                         np.max(py)\n",
    "                        ],\n",
    "                #XYXY_ABS: https://detectron2.readthedocs.io/en/latest/_modules/detectron2/structures/boxes.html\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": anno[\"category\"]\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "classes = [\"asset\"]\n",
    "colors = [(249, 180, 45)]\n",
    "\n",
    "#Register dataset and metadata\n",
    "for d in [\"train\", \"val\"]:\n",
    "    DatasetCatalog.register(d, lambda d=d: get_dataset_dicts(os.path.join(\"../NSO\", d)))\n",
    "    # Key-value mappingto interpret what’s in the dataset: names of classes, colors of classes\n",
    "    MetadataCatalog.get(d).thing_classes = classes\n",
    "    MetadataCatalog.get(d).thing_colors = colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise annotations of trainning dataset\n",
    "\n",
    "metadata = MetadataCatalog.get(\"train\")\n",
    "dataset_dicts = get_dataset_dicts(\"../NSO/train\")\n",
    "\n",
    "for d in random.sample(dataset_dicts, 1):\n",
    "    img = cv.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1, instance_mode=ColorMode.IMAGE)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    print(d[\"file_name\"])\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    #plt.imshow(out.get_image()[:, :, ::-1])#BGR to RGB\n",
    "    plt.imshow(out.get_image()[:, :, :])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c762196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise annotations of validation dataset\n",
    "\n",
    "metadata = MetadataCatalog.get(\"val\")\n",
    "dataset_dicts = get_dataset_dicts(\"../NSO/val\")\n",
    "\n",
    "for d in random.sample(dataset_dicts, 1):\n",
    "    img = cv.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1, instance_mode=ColorMode.IMAGE)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    print(d[\"file_name\"])\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    #plt.imshow(out.get_image()[:, :, ::-1])#BGR to RGB\n",
    "    plt.imshow(out.get_image()[:, :, :])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model configugation for training\n",
    "\n",
    "# initializes an empty config\n",
    "cfg = get_cfg()\n",
    "\n",
    "# add custom component to configuration\n",
    "# load values from API: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "#https://eidos-ai.medium.com/training-on-detectron2-with-a-validation-set-and-plot-loss-on-it-to-avoid-overfitting-6449418fbf4e\n",
    "cfg.DATASETS.TEST = (\"val\",) # should not be set for training: https://github.com/facebookresearch/detectron2/issues/951\n",
    "# initiate weights from model zoo\n",
    "# check points (.pth) = model's parameters and optimizer state to resume training or evalate performance\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\")\n",
    "# set learning rate\n",
    "cfg.SOLVER.BASE_LR = 0.005\n",
    "# Dataloader provides data to the model\n",
    "# Load data to GPU: the more data you put into the GPU memory, the less memory is available for the model (4 in Spacenet)\n",
    "# https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/33?page=2\n",
    "# https://stackoverflow.com/questions/53998282/how-does-the-number-of-workers-parameter-in-pytorch-dataloader-actually-work\n",
    "# Cluster warning: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# real batch size:  number of training examples utilized in one iteration.\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "# parameter  used to sample a subset of proposals coming out of RPN to calculate cls and reg loss during training\n",
    "# \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512). Spacenet uses 16 for 4 GPUs \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 16\n",
    "# Number of class = 1 (1 asset Type)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "# config not included SPACENET\n",
    "# adjust up if val mAP is still rising, adjust down if overfit/ 300 iterations seems good for toy dataset\n",
    "cfg.SOLVER.MAX_ITER = 50    \n",
    "# [] => do not decay learning rate\n",
    "#configuration specifies the iteration steps at which the learning rate is decreased during training.\n",
    "#cfg.SOLVER.STEPS = []\n",
    "#cfg.SOLVER.WARMUP_ITERS = 1000\n",
    "#cfg.SOLVER.GAMMA = 0.05\n",
    "#number of itteration after wich the validation set is used for evaluation\n",
    "#cfg.TEST.EVAL_PERIOD = 25 \n",
    "\n",
    "\n",
    "# Directory for (logs, configs, metrics, and model checkpoints)\n",
    "#cfg.OUTPUT_DIR = \"../NSO/output\n",
    "cfg.OUTPUT_DIR = \"../NSO/output_40tiles_005LR_4B_50iter\"\n",
    "# print configs \n",
    "print(cfg.dump())\n",
    "\n",
    "# Save your configurations for multi-GPU use\n",
    "with open(\"../NSO/NSOD2cfg_40tiles_005LR_4B_50iter.yaml\", \"w\") as file:\n",
    "    yaml.dump(cfg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0280273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AFTER TRAINING 1: Check if both work\n",
    "#https://github.com/rl02898/detectron2-spacenet/blob/master/detectron_demo.py\n",
    "\n",
    "# Inference configurations\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "# Overwriting configs for inference\n",
    "with open(\"../NSO/NSOD2cfg_40tiles_005LR_4B_50iter.yaml\", \"w\") as file:\n",
    "    yaml.dump(cfg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbcfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AFTER TRAINING 2: : Check if both work\n",
    "#https://russland.medium.com/using-detectron2-for-instance-segmentation-on-the-spacenet-dataset-94338f739cd0\n",
    "\n",
    "# Inference configurations\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "predictor = DefaultPredictor(cfg)\n",
    "metadata = MetadataCatalog.get(\"train\")\n",
    "\n",
    "# Overwriting configs for inference\n",
    "with open(\"../NSO/NSOD2cfg_40tiles_005LR_4B_50iter.yaml\", \"w\") as file:\n",
    "    yaml.dump(cfg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da7f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4beb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "784602db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77a7c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cm/local/apps/gcc/11.2.0/lib:/cm/local/apps/gcc/11.2.0/lib64:/cm/local/apps/gcc/11.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['LD_LIBRARY_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f8b416b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475b08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b7ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
