{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor, hooks\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset, print_csv_format\n",
    "\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import detectron2\n",
    "import pandas as pd\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from train import get_dataset_dicts, get_dataset_dicts_with_regions, random_visu, setup_cfg, MyTrainer, load_json_arr, find_best_model\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOLDER PATH ###\n",
    "\n",
    "dataset_eval = \"test\"\n",
    "dataset_eval_output = \"test_output_via_region_data\"\n",
    "annotation_json = \"via_region_data.json\"\n",
    "results_file = \"results.json\"\n",
    "experiment = \"../NSO/output/lr0001_BS32\"\n",
    "conf_path = \"../NSO/output/lr0001_BS32/NSOD2cfg_1000_169r_1000pix_noBT_0001LR_32BS_128BSPI_eval.yaml\"\n",
    "model_path = experiment + \"/model_17.pth\"\n",
    "filter_empty_annot = True\n",
    "\n",
    "\n",
    "annotation_output = os.path.splitext(annotation_json)[0]\n",
    "out_dir = os.path.join(experiment, dataset_eval_output, annotation_output)\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    print (\"The test directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Indicate to D2 how to read dataset if not in coco format: ###\n",
    "\n",
    "# D2 metadata: name of classes and colors of annotations\n",
    "classes = [\"container\"]\n",
    "colors = [(249, 180, 45)]\n",
    "\n",
    "# Register dataset and metadata\n",
    "for d in [\"train\", \"val\", \"test\"]:\n",
    "    DatasetCatalog.register(d, lambda d=d:\n",
    "                            get_dataset_dicts(os.path.join(\"../Satellite\", d), annotation_json))\n",
    "    # Key-value mapping to interpret whatâ€™s in the dataset: names of classes, colors of classes\n",
    "    MetadataCatalog.get(d).thing_classes = classes\n",
    "    MetadataCatalog.get(d).thing_colors = colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config ../NSO/output/lr0001_BS32/NSOD2cfg_1000_169r_1000pix_noBT_0001LR_32BS_128BSPI_eval.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
      "/scistor/ivm/jpl204/miniconda3/envs/nso_fix/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/scistor/ivm/jpl204/miniconda3/envs/nso_fix/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.095\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.186\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.077\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.153\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.124\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.130\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.130\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.025\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.175\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.017\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.104\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.082\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.087\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.087\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.142\n",
      "Results dumped\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "# keep those 2 to avoid errors of MIN_SIZE_TRAIN = 800 and resize not found\n",
    "#change in config: \n",
    "#MIN_SIZE_TRAIN: !!python/tuple\n",
    "#- 1000\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (1000,)\n",
    "cfg.RESIZE= False\n",
    "cfg.merge_from_file(conf_path)\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (1000,)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = model_path\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = filter_empty_annot\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "evaluator = COCOEvaluator(dataset_eval, output_dir=out_dir)\n",
    "\n",
    "val_loader = build_detection_test_loader(cfg, dataset_eval)\n",
    "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "\n",
    "with open(os.path.join(out_dir, results_file), 'w') as f:\n",
    "    json.dump(results, f)\n",
    "print(\"Results dumped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_dir = '../NSO/output/lr001_BS4_empty-annot-50/eval'\n",
    "# Load the JSON pred file into a DataFrame\n",
    "df_pred = pd.read_json(os.path.join(out_dir, 'coco_instances_results.json'))\n",
    "df_pred = df_pred[df_pred['score'] > 0.5] \n",
    "\n",
    "# Load the JSON ground truth file into a DataFrame\n",
    "\n",
    "# CHANGE VAL OR TEST\n",
    "# Extract the \"images\" key and create a DataFrame from it\n",
    "\n",
    "# Ground thruth OG\n",
    "df_truth_og = pd.read_json(os.path.join(os.path.join(\"../\", dataset_eval), annotation_json))\n",
    "with open(os.path.join(out_dir, 'test_coco_format.json'), 'r') as j:\n",
    "     data = json.loads(j.read())\n",
    "images_data = data.get('images', [])  # Get the list of \"images\" objects\n",
    "annotation_data = data.get('annotations', [])  # Get the list of \"categories\" objects\n",
    "# Create a DataFrame from the valid \"images\" objects\n",
    "df_images = pd.DataFrame(images_data)\n",
    "df_annotations = pd.DataFrame(annotation_data)\n",
    "#Drop duplicates: where are they comming from???? =>val_coco is automatically done by D2 check if where nitially in dataset\n",
    "df_annotations = df_annotations.drop_duplicates(subset=['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth = pd.merge(df_images, df_annotations, left_on='id', right_on='image_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop duplicates => why are they there?\n",
    "#df_truth = df_truth.drop_duplicates(subset=['bbox'])\n",
    "df_eval = pd.merge(df_truth, df_pred, left_on='id_x', right_on='image_id', how='left')\n",
    "# df_eval = df_eval.drop_duplicates(subset=['segmentation_x'])\n",
    "# #draop images with several annotation, just need to keep one\n",
    "df_eval = df_eval.drop_duplicates(subset=['id_x'])\n",
    "df_eval['truth'] = None\n",
    "df_eval['pred'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['truth'] = np.where(df_eval.bbox_x.isnull(), 0, 1)\n",
    "df_eval['pred'] = np.where(df_eval.score.isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fscore:  0.7901234567901235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#ravel to flatten conf matrix to 1D insted of 2D\n",
    "tn, fp, fn, tp = confusion_matrix(df_eval['truth'], df_eval['pred']).ravel()\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "print (\"Fscore: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to <_io.TextIOWrapper name='../NSO/output/lr0001_BS32/test_output_via_region_data/via_region_data/metrics_tiles.json' mode='w' encoding='UTF-8'>\n",
      "0.7901234567901235\n"
     ]
    }
   ],
   "source": [
    "# Convert int64 values to native Python int\n",
    "tn = int(tn)\n",
    "fp = int(fp)\n",
    "fn = int(fn)\n",
    "tp = int(tp)\n",
    "\n",
    "metrics_dict = {\n",
    "    'tn': tn,\n",
    "    'fp': fp,\n",
    "    'fn': fn,\n",
    "    'tp': tp,\n",
    "    'precision_score': precision_score,\n",
    "    'recall_score': recall_score,\n",
    "    'f1_score': f1_score\n",
    "}\n",
    "\n",
    "# Define the file path\n",
    "json_metrics = 'metrics_tiles.json'\n",
    "json_path = os.path.join(out_dir, json_metrics)\n",
    "\n",
    "# Save the metrics to a JSON file\n",
    "with open(json_path, 'w') as json_file:\n",
    "    json.dump(metrics_dict, json_file, indent=4)\n",
    "\n",
    "print(f'Metrics saved to {json_file}')\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
