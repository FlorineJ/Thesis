{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb6c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import detectron2\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d1201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 1 0 <torch.cuda.device object at 0x1555505f3d30> NVIDIA A30\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A30\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU information\n",
    "print (torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.device(0)\n",
    "       , torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional info when using CUDA\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62daceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate to D2 how to read dataset if not in coco format:\n",
    "\n",
    "def get_dataset_dicts(img_dir):\n",
    "    \"\"\"Function that tells detectron2 know how to obtain a custom datasets by specifying it similarly to COCO’s annotations: see https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html \n",
    "\n",
    "    Args:\n",
    "        img_dir (str): path to the directory containing the images and annotations .json file \n",
    "\n",
    "    Returns:\n",
    "        list: list of dictionaries containing the information of the dataset\n",
    "    \"\"\"\n",
    "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
    "    with open(json_file) as f:\n",
    "        imgs_anns = json.load(f)\n",
    "    \n",
    "    dataset_dicts = []\n",
    "    for idx, annots in enumerate(imgs_anns.values()):\n",
    "        record = {}\n",
    "        \n",
    "        filename = os.path.join(img_dir, annots[\"filename\"])\n",
    "        height, width = cv.imread(filename).shape[:2]\n",
    "        \n",
    "        #Info on the tile .png file\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        \n",
    "        #info on annotation\n",
    "        annotations = annots[\"regions\"]\n",
    "        objs = []\n",
    "        for _, anno in annotations.items():\n",
    "            assert not anno[\"region_attributes\"]\n",
    "            anno = anno[\"shape_attributes\"]\n",
    "            px = anno[\"all_points_x\"]\n",
    "            py = anno[\"all_points_y\"]\n",
    "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px),\n",
    "                         np.min(py),\n",
    "                         np.max(px),\n",
    "                         np.max(py)\n",
    "                        ],\n",
    "                #XYXY_ABS: https://detectron2.readthedocs.io/en/latest/_modules/detectron2/structures/boxes.html\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": anno[\"category\"]\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "classes = [\"asset\"]\n",
    "colors = [(249, 180, 45)]\n",
    "\n",
    "#Register dataset and metadata\n",
    "for d in [\"train\", \"val\"]:\n",
    "    DatasetCatalog.register(d, lambda d=d: get_dataset_dicts(os.path.join(\"../NSO\", d)))\n",
    "    # Key-value mappingto interpret what’s in the dataset: names of classes, colors of classes\n",
    "    MetadataCatalog.get(d).thing_classes = classes\n",
    "    MetadataCatalog.get(d).thing_colors = colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6828099c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m metadata \u001b[38;5;241m=\u001b[39m MetadataCatalog\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m dataset_dicts \u001b[38;5;241m=\u001b[39m get_dataset_dicts(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../NSO/train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      8\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mimread(d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m     visualizer \u001b[38;5;241m=\u001b[39m Visualizer(img[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], metadata\u001b[38;5;241m=\u001b[39mmetadata, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, instance_mode\u001b[38;5;241m=\u001b[39mColorMode\u001b[38;5;241m.\u001b[39mIMAGE)\n",
      "File \u001b[0;32m~/miniconda3/envs/nso/lib/python3.9/random.py:449\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    447\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[1;32m    451\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "# Visualise annotations of trainning dataset\n",
    "# Urgent! Prob with several annot on the same tile...\n",
    "\n",
    "metadata = MetadataCatalog.get(\"train\")\n",
    "dataset_dicts = get_dataset_dicts(\"../NSO/train\")\n",
    "\n",
    "for d in random.sample(dataset_dicts, 1):\n",
    "    img = cv.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1, instance_mode=ColorMode.IMAGE)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    print(d[\"file_name\"])\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1]) #BGR to RGB\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "843cb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN_BENCHMARK: false\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: true\n",
      "  FILTER_EMPTY_ANNOTATIONS: true\n",
      "  NUM_WORKERS: 2\n",
      "  REPEAT_THRESHOLD: 0.0\n",
      "  SAMPLER_TRAIN: TrainingSampler\n",
      "DATASETS:\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
      "  PROPOSAL_FILES_TEST: []\n",
      "  PROPOSAL_FILES_TRAIN: []\n",
      "  TEST: []\n",
      "  TRAIN:\n",
      "  - train\n",
      "GLOBAL:\n",
      "  HACK: 1.0\n",
      "INPUT:\n",
      "  CROP:\n",
      "    ENABLED: false\n",
      "    SIZE:\n",
      "    - 0.9\n",
      "    - 0.9\n",
      "    TYPE: relative_range\n",
      "  FORMAT: BGR\n",
      "  MASK_FORMAT: polygon\n",
      "  MAX_SIZE_TEST: 1333\n",
      "  MAX_SIZE_TRAIN: 1333\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN:\n",
      "  - 640\n",
      "  - 672\n",
      "  - 704\n",
      "  - 736\n",
      "  - 768\n",
      "  - 800\n",
      "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
      "  RANDOM_FLIP: horizontal\n",
      "MODEL:\n",
      "  ANCHOR_GENERATOR:\n",
      "    ANGLES:\n",
      "    - - -90\n",
      "      - 0\n",
      "      - 90\n",
      "    ASPECT_RATIOS:\n",
      "    - - 0.5\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "    NAME: DefaultAnchorGenerator\n",
      "    OFFSET: 0.0\n",
      "    SIZES:\n",
      "    - - 32\n",
      "    - - 64\n",
      "    - - 128\n",
      "    - - 256\n",
      "    - - 512\n",
      "  BACKBONE:\n",
      "    FREEZE_AT: 2\n",
      "    NAME: build_resnet_fpn_backbone\n",
      "  DEVICE: cuda\n",
      "  FPN:\n",
      "    FUSE_TYPE: sum\n",
      "    IN_FEATURES:\n",
      "    - res2\n",
      "    - res3\n",
      "    - res4\n",
      "    - res5\n",
      "    NORM: ''\n",
      "    OUT_CHANNELS: 256\n",
      "  KEYPOINT_ON: false\n",
      "  LOAD_PROPOSALS: false\n",
      "  MASK_ON: true\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  PANOPTIC_FPN:\n",
      "    COMBINE:\n",
      "      ENABLED: true\n",
      "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
      "      OVERLAP_THRESH: 0.5\n",
      "      STUFF_AREA_LIMIT: 4096\n",
      "    INSTANCE_LOSS_WEIGHT: 1.0\n",
      "  PIXEL_MEAN:\n",
      "  - 103.53\n",
      "  - 116.28\n",
      "  - 123.675\n",
      "  PIXEL_STD:\n",
      "  - 1.0\n",
      "  - 1.0\n",
      "  - 1.0\n",
      "  PROPOSAL_GENERATOR:\n",
      "    MIN_SIZE: 0\n",
      "    NAME: RPN\n",
      "  RESNETS:\n",
      "    DEFORM_MODULATED: false\n",
      "    DEFORM_NUM_GROUPS: 1\n",
      "    DEFORM_ON_PER_STAGE:\n",
      "    - false\n",
      "    - false\n",
      "    - false\n",
      "    - false\n",
      "    DEPTH: 50\n",
      "    NORM: FrozenBN\n",
      "    NUM_GROUPS: 1\n",
      "    OUT_FEATURES:\n",
      "    - res2\n",
      "    - res3\n",
      "    - res4\n",
      "    - res5\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: true\n",
      "    WIDTH_PER_GROUP: 64\n",
      "  RETINANET:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_WEIGHTS: &id002\n",
      "    - 1.0\n",
      "    - 1.0\n",
      "    - 1.0\n",
      "    - 1.0\n",
      "    FOCAL_LOSS_ALPHA: 0.25\n",
      "    FOCAL_LOSS_GAMMA: 2.0\n",
      "    IN_FEATURES:\n",
      "    - p3\n",
      "    - p4\n",
      "    - p5\n",
      "    - p6\n",
      "    - p7\n",
      "    IOU_LABELS:\n",
      "    - 0\n",
      "    - -1\n",
      "    - 1\n",
      "    IOU_THRESHOLDS:\n",
      "    - 0.4\n",
      "    - 0.5\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NORM: ''\n",
      "    NUM_CLASSES: 80\n",
      "    NUM_CONVS: 4\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "    SMOOTH_L1_LOSS_BETA: 0.1\n",
      "    TOPK_CANDIDATES_TEST: 1000\n",
      "  ROI_BOX_CASCADE_HEAD:\n",
      "    BBOX_REG_WEIGHTS:\n",
      "    - &id001\n",
      "      - 10.0\n",
      "      - 10.0\n",
      "      - 5.0\n",
      "      - 5.0\n",
      "    - - 20.0\n",
      "      - 20.0\n",
      "      - 10.0\n",
      "      - 10.0\n",
      "    - - 30.0\n",
      "      - 30.0\n",
      "      - 15.0\n",
      "      - 15.0\n",
      "    IOUS:\n",
      "    - 0.5\n",
      "    - 0.6\n",
      "    - 0.7\n",
      "  ROI_BOX_HEAD:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: *id001\n",
      "    CLS_AGNOSTIC_BBOX_REG: false\n",
      "    CONV_DIM: 256\n",
      "    FC_DIM: 1024\n",
      "    NAME: FastRCNNConvFCHead\n",
      "    NORM: ''\n",
      "    NUM_CONV: 0\n",
      "    NUM_FC: 2\n",
      "    POOLER_RESOLUTION: 7\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "    TRAIN_ON_PRED_BOXES: false\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 4\n",
      "    IN_FEATURES:\n",
      "    - p2\n",
      "    - p3\n",
      "    - p4\n",
      "    - p5\n",
      "    IOU_LABELS:\n",
      "    - 0\n",
      "    - 1\n",
      "    IOU_THRESHOLDS:\n",
      "    - 0.5\n",
      "    NAME: StandardROIHeads\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NUM_CLASSES: 1\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    PROPOSAL_APPEND_GT: true\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_DIMS:\n",
      "    - 512\n",
      "    - 512\n",
      "    - 512\n",
      "    - 512\n",
      "    - 512\n",
      "    - 512\n",
      "    - 512\n",
      "    - 512\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
      "    NAME: KRCNNConvDeconvUpsampleHead\n",
      "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n",
      "    NUM_KEYPOINTS: 17\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  ROI_MASK_HEAD:\n",
      "    CLS_AGNOSTIC_MASK: false\n",
      "    CONV_DIM: 256\n",
      "    NAME: MaskRCNNConvUpsampleHead\n",
      "    NORM: ''\n",
      "    NUM_CONV: 4\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  RPN:\n",
      "    BATCH_SIZE_PER_IMAGE: 256\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: *id002\n",
      "    BOUNDARY_THRESH: -1\n",
      "    CONV_DIMS:\n",
      "    - -1\n",
      "    HEAD_NAME: StandardRPNHead\n",
      "    IN_FEATURES:\n",
      "    - p2\n",
      "    - p3\n",
      "    - p4\n",
      "    - p5\n",
      "    - p6\n",
      "    IOU_LABELS:\n",
      "    - 0\n",
      "    - -1\n",
      "    - 1\n",
      "    IOU_THRESHOLDS:\n",
      "    - 0.3\n",
      "    - 0.7\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOPK_TEST: 1000\n",
      "    POST_NMS_TOPK_TRAIN: 1000\n",
      "    PRE_NMS_TOPK_TEST: 1000\n",
      "    PRE_NMS_TOPK_TRAIN: 2000\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "  SEM_SEG_HEAD:\n",
      "    COMMON_STRIDE: 4\n",
      "    CONVS_DIM: 128\n",
      "    IGNORE_VALUE: 255\n",
      "    IN_FEATURES:\n",
      "    - p2\n",
      "    - p3\n",
      "    - p4\n",
      "    - p5\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NAME: SemSegFPNHead\n",
      "    NORM: GN\n",
      "    NUM_CLASSES: 54\n",
      "  WEIGHTS: https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x/137260431/model_final_a54504.pkl\n",
      "OUTPUT_DIR: ../NSO/output\n",
      "SEED: -1\n",
      "SOLVER:\n",
      "  AMP:\n",
      "    ENABLED: false\n",
      "  BASE_LR: 0.005\n",
      "  BIAS_LR_FACTOR: 1.0\n",
      "  CHECKPOINT_PERIOD: 5000\n",
      "  CLIP_GRADIENTS:\n",
      "    CLIP_TYPE: value\n",
      "    CLIP_VALUE: 1.0\n",
      "    ENABLED: false\n",
      "    NORM_TYPE: 2.0\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 16\n",
      "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
      "  MAX_ITER: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: false\n",
      "  REFERENCE_WORLD_SIZE: 0\n",
      "  STEPS: []\n",
      "  WARMUP_FACTOR: 0.001\n",
      "  WARMUP_ITERS: 1000\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: null\n",
      "  WEIGHT_DECAY_NORM: 0.0\n",
      "TEST:\n",
      "  AUG:\n",
      "    ENABLED: false\n",
      "    FLIP: true\n",
      "    MAX_SIZE: 4000\n",
      "    MIN_SIZES:\n",
      "    - 400\n",
      "    - 500\n",
      "    - 600\n",
      "    - 700\n",
      "    - 800\n",
      "    - 900\n",
      "    - 1000\n",
      "    - 1100\n",
      "    - 1200\n",
      "  DETECTIONS_PER_IMAGE: 100\n",
      "  EVAL_PERIOD: 50\n",
      "  EXPECTED_RESULTS: []\n",
      "  KEYPOINT_OKS_SIGMAS: []\n",
      "  PRECISE_BN:\n",
      "    ENABLED: false\n",
      "    NUM_ITER: 200\n",
      "VERSION: 2\n",
      "VIS_PERIOD: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting up model configugation for training\n",
    "\n",
    "# initializes an empty config\n",
    "cfg = get_cfg()\n",
    "\n",
    "# add custom component to configuration\n",
    "# load values from API: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = () # should not be set for training: https://github.com/facebookresearch/detectron2/issues/951\n",
    "# initiate weights from model zoo\n",
    "# check points (.pth) = model's parameters and optimizer state to resume training or evalate performance\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\")\n",
    "# set learning rate\n",
    "cfg.SOLVER.BASE_LR = 0.005\n",
    "# Dataloader provides data to the model\n",
    "# Load data to GPU: the more data you put into the GPU memory, the less memory is available for the model (4 in Spacenet)\n",
    "# https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/33?page=2\n",
    "# https://stackoverflow.com/questions/53998282/how-does-the-number-of-workers-parameter-in-pytorch-dataloader-actually-work\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# real batch size:  number of training examples utilized in one iteration.\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "# parameter  used to sample a subset of proposals coming out of RPN to calculate cls and reg loss during training\n",
    "# \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512). Spacenet uses 16 for 4 GPUs \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 4\n",
    "# Number of class = 1 (1 asset Type)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "# config not included SPACENET\n",
    "# adjust up if val mAP is still rising, adjust down if overfit/ 300 iterations seems good for toy dataset\n",
    "cfg.SOLVER.MAX_ITER = 100    \n",
    "# [] => do not decay learning rate \n",
    "cfg.SOLVER.STEPS = []\n",
    "#cfg.SOLVER.WARMUP_ITERS = 1000\n",
    "#cfg.SOLVER.GAMMA = 0.05\n",
    "#number of itteration after wich the validation set is used for evaluation\n",
    "cfg.TEST.EVAL_PERIOD = 50 \n",
    "\n",
    "\n",
    "# Directory for (logs, configs, metrics, and model checkpoints)\n",
    "cfg.OUTPUT_DIR = \"../NSO/output\"\n",
    "# print configs \n",
    "print(cfg.dump())\n",
    "\n",
    "# Save your configurations for multi-GPU use\n",
    "with open(\"../NSO/NSOD2cfg_Abcoude.yaml\", \"w\") as file:\n",
    "    yaml.dump(cfg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd43d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/08 19:10:15 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/08 19:10:15 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 6 images left.\n",
      "\u001b[32m[03/08 19:10:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[03/08 19:10:15 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[03/08 19:10:15 d2.data.common]: \u001b[0mSerializing 6 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/08 19:10:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.00 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/08 19:10:15 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[03/08 19:10:37 d2.utils.events]: \u001b[0m eta: 0:01:07  iter: 19  total_loss: 1.879  loss_cls: 0.4876  loss_box_reg: 0.6871  loss_mask: 0.6682  loss_rpn_cls: 0.05275  loss_rpn_loc: 0.00917  time: 1.0492  data_time: 0.4279  lr: 0.00095405  max_mem: 10845M\n",
      "\u001b[32m[03/08 19:10:58 d2.utils.events]: \u001b[0m eta: 0:00:50  iter: 39  total_loss: 1.341  loss_cls: 0.1463  loss_box_reg: 0.7132  loss_mask: 0.4034  loss_rpn_cls: 0.00913  loss_rpn_loc: 0.00807  time: 1.0603  data_time: 0.4290  lr: 0.0019531  max_mem: 10845M\n",
      "\u001b[32m[03/08 19:11:19 d2.utils.events]: \u001b[0m eta: 0:00:33  iter: 59  total_loss: 0.6448  loss_cls: 0.05474  loss_box_reg: 0.3627  loss_mask: 0.2291  loss_rpn_cls: 0.002824  loss_rpn_loc: 0.005575  time: 1.0608  data_time: 0.4223  lr: 0.002952  max_mem: 10845M\n",
      "\u001b[32m[03/08 19:11:45 d2.utils.events]: \u001b[0m eta: 0:00:16  iter: 79  total_loss: 0.5045  loss_cls: 0.03462  loss_box_reg: 0.2912  loss_mask: 0.1512  loss_rpn_cls: 0.001981  loss_rpn_loc: 0.004243  time: 1.1219  data_time: 0.6568  lr: 0.0039511  max_mem: 10845M\n",
      "\u001b[32m[03/08 19:12:10 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 99  total_loss: 0.4618  loss_cls: 0.03956  loss_box_reg: 0.2956  loss_mask: 0.1152  loss_rpn_cls: 0.002082  loss_rpn_loc: 0.004317  time: 1.1272  data_time: 0.5055  lr: 0.0049501  max_mem: 10845M\n",
      "\u001b[32m[03/08 19:12:10 d2.engine.hooks]: \u001b[0mOverall training speed: 98 iterations in 0:01:50 (1.1272 s / it)\n",
      "\u001b[32m[03/08 19:12:10 d2.engine.hooks]: \u001b[0mTotal training time: 0:01:51 (0:00:01 on hooks)\n"
     ]
    }
   ],
   "source": [
    "# Single GPU Training\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "# DefaultTrainer\" which contains pre-defined default logic for standard training workflow.\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "#Expect skip loading parameter because: The warning message is expected. Because your dataset has different number of classes from the pre-trained model, a few pre-trained weights have to be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a7b68",
   "metadata": {},
   "source": [
    "total_loss: This is a weighted sum of the following individual losses calculated during the iteration. By default, the weights are all one.\n",
    "\n",
    "loss_cls: Classification loss in the ROI head. Measures the loss for box classification, i.e., how good the model is at labelling a predicted box with the correct class.\n",
    "\n",
    "loss_box_reg: Localisation loss in the ROI head. Measures the loss for box localisation (predicted location vs true location).\n",
    "\n",
    "loss_rpn_cls: Classification loss in the Region Proposal Network. Measures the \"objectness\" loss, i.e., how good the RPN is at labelling the anchor boxes as foreground or background.\n",
    "\n",
    "loss_rpn_loc: Localisation loss in the Region Proposal Network. Measures the loss for localisation of the predicted regions in the RPN.\n",
    "\n",
    "loss_mask: Mask loss in the Mask head. Measures how \"correct\" the predicted binary masks are.\n",
    "\n",
    "For more details on the losses (1) and (2), take a look at the Fast R-CNN paper and the code.\n",
    "\n",
    "For more details on the losses (3) and (4), take a look at the Faster R-CNN paper and the code.\n",
    "\n",
    "For more details on the loss (5), take a look at the Mask R-CNN paper and the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
