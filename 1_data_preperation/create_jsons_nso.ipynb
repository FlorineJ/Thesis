{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38c314c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import geojson\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# custom functions\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.functions import grab_certain_file\n",
    "# TODO delete functions.py in data_preperation folder. Use main utils instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9e91b",
   "metadata": {},
   "source": [
    "'''\n",
    "Read NSO tiles and annotations geojsons, convert lat/lon of tile to pixel coordinates and save pixel coordinates into\n",
    ".json file If more than on .json file can be saved as one via_regions.json.\n",
    "Source:  https://github.com/rl02898/detectron2-spacenet. JDP Edits:some and saving json with origins of tile in jsons toallow stiching back\n",
    "of the tiles.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37186f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat train val and test folder in NSO directory\n",
    "root_path = '../NSO'\n",
    "folders = ['train','test','val']\n",
    "for folder in folders:\n",
    "    os.mkdir(os.path.join(root_path,folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7343cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset and set a random seed for reproducibility of the splits for next script\n",
    "# TODO: path nso_train and nso_geojson to tiling.py output => Better move output of tiling to those 2 folder in NSO/ \n",
    "\n",
    "RANDOM_SEED = 560\n",
    "\n",
    "# Change name to dataset...\n",
    "nso_train = \"../NSO/nso_all\"\n",
    "nso_geojson = \"../NSO/geojsons_annotations_nso\"\n",
    "#nso_images = grab_certain_file(\".tif\", nso_train)\n",
    "#train, val = train_test_split(nso_images, test_size=0.2, random_state=RANDOM_SEED)\n",
    "nso_images = ['1_6000_23000.tif', '1_7000_23000.tif', '1_7000_24000.tif']\n",
    "train = nso_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f447e756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating JSONs for Detectron2 on NSO: 100%|██████████| 3/3 [00:00<00:00, 120.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "\n",
    "# Create an empty dictionary to store the training set of annotations and their pixel coordinates\n",
    "train_dict = {}\n",
    "\n",
    "# Loop over each image in the training set\n",
    "for file in tqdm(train, desc=\"Creating JSONs for Detectron2 on NSO\", ncols=150, bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "    file_path = os.path.join(nso_train, file)\n",
    "    img_id = file.split(\".tif\")[0]\n",
    "    geojson_path = os.path.join(nso_geojson, f\"{img_id}.geojson\")\n",
    "    \n",
    "    #Not all tiles have annotations, thus:\n",
    "    if os.path.exists(geojson_path):\n",
    "        \n",
    "        # Load the geojson in gj\n",
    "        with open(geojson_path) as f:\n",
    "            gj = geojson.load(f)\n",
    "        \n",
    "        # Create a dictionary to store the regions (annotations spatial features) for the image\n",
    "        regions = {}\n",
    "        num_buildings = len(gj[\"features\"])\n",
    "        \n",
    "        # Open the image with gdal to get pixel size and origin if feature exists\n",
    "        if num_buildings > 0:\n",
    "            gdal_image = gdal.Open(file_path)\n",
    "            \n",
    "            # Get the pixel width and height(0.5 for nso) and the origin coordinates\n",
    "            #https://www.gis.usu.edu/~chrisg/python/2009/lectures/ospy_slides4.pdf\n",
    "            pixel_width, pixel_height = gdal_image.GetGeoTransform()[1], gdal_image.GetGeoTransform()[5]\n",
    "            originX, originY = gdal_image.GetGeoTransform()[0], gdal_image.GetGeoTransform()[3]\n",
    "            \n",
    "            # Loop over each building/assets in the image\n",
    "            for i in range(num_buildings):\n",
    "                \n",
    "                # Get the polygon points for the asset\n",
    "                #https://stackoverflow.com/questions/23306653/python-accessing-nested-json-data\n",
    "                points = gj[\"features\"][i][\"geometry\"][\"coordinates\"][0]\n",
    "                \n",
    "                # If there is only one point, unwarp it=>check\n",
    "                if len(points) == 1:\n",
    "                    points = points[0]\n",
    "\n",
    "                #Empty lists to store pixel coordinates\n",
    "                all_points_x, all_points_y = [], []\n",
    "                \n",
    "                # Convert the lat/long points to pixel coordinates by substacting origin\n",
    "                for j in range(len(points)):\n",
    "                    all_points_x.append(int(round((points[j][0] - originX) / pixel_width)))\n",
    "                    all_points_y.append(int(round((points[j][1] - originY) / pixel_height)))\n",
    "                    \n",
    "                # Create a dictionary to store the asset footprint\n",
    "                regions[str(i)] = {\"shape_attributes\":\n",
    "                                       {\"name\": \"polygon\",\n",
    "                                        \"all_points_x\": all_points_x,\n",
    "                                        \"all_points_y\": all_points_y,\n",
    "                                        \"category\": 0\n",
    "                                       },\n",
    "                                   \"region_attributes\": {}\n",
    "                                  }\n",
    "        #Should probably save origin x and y here but we still have the og tiles and imgid and allow to stich tiles back together\n",
    "        #TODO: same for tiles without annot.Eg create json with empty regions\n",
    "        dictionary = {\"file_ref\": '',\n",
    "                      \"size\": os.path.getsize(file_path),\n",
    "                      \"filename\": file.replace(\".tif\", \".png\"),\n",
    "                      \"base64_img_data\": '',\n",
    "                      \"file_attributes\": {},\n",
    "                      \"regions\": regions,\n",
    "                      \"origin_x\": originX,\n",
    "                      \"origin_y\": originY\n",
    "                     }\n",
    "\n",
    "        train_dict[file.replace(\".tif\", \".png\")] = dictionary\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "with open(\"../NSO/train/nso.json\", \"w\") as f:\n",
    "    json.dump(train_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f301403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating JSONs for Detectron2 on 1_Rio_val: 100%|██████████| 1106/1106 [00:00<00:00, 8542.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#Validation set\n",
    "val_dict = {}\n",
    "\n",
    "for file in tqdm(val, desc=\"Creating JSONs for Detectron2 on 1_Rio_val\", ncols=150, bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "    file_path = os.path.join(nso_train, file)\n",
    "    img_id = file.split(\".tif\")[0]\n",
    "    geojson_path = os.path.join(nso_geojson, f\"{img_id}.geojson\")\n",
    "    if os.path.exists(geojson_path): \n",
    "        with open(geojson_path) as f:\n",
    "            gj = geojson.load(f)\n",
    "\n",
    "        regions = {}\n",
    "        num_buildings = len(gj[\"features\"])\n",
    "        if num_buildings > 0:\n",
    "            gdal_image = gdal.Open(file_path)\n",
    "            pixel_width, pixel_height = gdal_image.GetGeoTransform()[1], gdal_image.GetGeoTransform()[5]\n",
    "            originX, originY = gdal_image.GetGeoTransform()[0], gdal_image.GetGeoTransform()[3]\n",
    "\n",
    "            for i in range(num_buildings):\n",
    "                points = gj[\"features\"][i][\"geometry\"][\"coordinates\"][0]\n",
    "                if len(points) == 1:\n",
    "                    points = points[0]\n",
    "\n",
    "                all_points_x, all_points_y = [], []\n",
    "                for j in range(len(points)):\n",
    "                    all_points_x.append(int(round((points[j][0] - originX) / pixel_width)))\n",
    "                    all_points_y.append(int(round((points[j][1] - originY) / pixel_height)))\n",
    "\n",
    "                regions[str(i)] = {\"shape_attributes\":\n",
    "                                       {\"name\": \"polygon\",\n",
    "                                        \"all_points_x\": all_points_x,\n",
    "                                        \"all_points_y\": all_points_y,\n",
    "                                        \"category\": 0\n",
    "                                       },\n",
    "                                   \"region_attributes\": {}\n",
    "                                  }\n",
    "\n",
    "        dictionary = {\"file_ref\": '',\n",
    "                      \"size\": os.path.getsize(file_path),\n",
    "                      \"filename\": file.replace(\".tif\", \".png\"),\n",
    "                      \"base64_img_data\": '',\n",
    "                      \"file_attributes\": {},\n",
    "                      \"regions\": regions,\n",
    "                      \"origin_x\": originX,\n",
    "                      \"origin_y\": originY\n",
    "                     }\n",
    "\n",
    "        val_dict[file.replace(\".tif\", \".png\")] = dictionary\n",
    "\n",
    "with open(\"../NSO/val/nso.json\", \"w\") as f:\n",
    "    json.dump(val_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a964211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single via_region_data training dataset => can be skipped if only one .json file.=>But then change file name\n",
    "jsons = [\"../NSO/train/nso.json\"]\n",
    "\n",
    "result = {}\n",
    "for file in jsons:\n",
    "    with open(file, \"r\") as f:\n",
    "        loaded = json.load(f)\n",
    "        \n",
    "    #https://realpython.com/iterate-through-dictionary-python/\n",
    "    for key, value in loaded.items():\n",
    "        result[key] = value\n",
    "\n",
    "with open(\"../NSO/train/via_region_data.json\", \"w\") as file:\n",
    "    json.dump(result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4426d33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating JSONs\n"
     ]
    }
   ],
   "source": [
    "# Create via_region JSON for entire validation dataset => can be skipped if only one .json file.=>But then change file name\n",
    "jsons = [\"../NSO/val/nso.json\"]\n",
    "\n",
    "result = {}\n",
    "for file in jsons:\n",
    "    with open(file, \"r\") as f:\n",
    "        loaded = json.load(f)\n",
    "    for key, value in loaded.items():\n",
    "        result[key] = value\n",
    "\n",
    "with open(\"../NSO/val/via_region_data.json\", \"w\") as file:\n",
    "    json.dump(result, file)\n",
    "\n",
    "print(\"Done creating JSONs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575969c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do the same 2 cells when there is a test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
