{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import geojson\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# custom functions\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.functions import grab_certain_file\n",
    "from utils.create_jsons import geojson_to_json_pix_coords\n",
    "# TODO delete functions.py in data_preperation folder. Use main utils instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ARGS function ###\n",
    "\n",
    "path = \"../Satellite\"\n",
    "train_path = os.path.join(path, \"train\")\n",
    "test_path = os.path.join(path, \"test\")\n",
    "val_path = os.path.join(path, \"val\")\n",
    "geojson_path = os.path.join(path, \"split_geojsons\")\n",
    "small_tiles_path = os.path.join(path, \"small_tiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144507ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset and set a random seed for reproducibility of the splits for next script\n",
    "\n",
    "RANDOM_SEED = 560\n",
    "\n",
    "# Create JSONs for Detectron2 NO test set\n",
    "#nso_images = grab_certain_file(\".tif\", small_tiles_path)\n",
    "#train, val = train_test_split(nso_images, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# Create JSONs for Detectron2 WITH test set\n",
    "# Find this function in utils --> functions.py\n",
    "images = grab_certain_file(\".tif\", small_tiles_path)\n",
    "train, test = train_test_split(images, test_size=0.20, random_state=RANDOM_SEED)\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the geojson files to pixel coordinates, this is needed for detectron2 to be able to read the locations\n",
    "\n",
    "def geojson_to_json_pix_coords(dataset_split, small_tiles_path, geojson_path, dataset_path):\n",
    "    \"\"\"\n",
    "    Converts geojson annotations to JSON format with pixel coordinates.\n",
    "\n",
    "    Args:\n",
    "        dataset_split (list): List of image files in the dataset split: train, test, or val.\n",
    "        small_tiles_path (str): Path to the directory containing the small tiles (.tif).\n",
    "        geojson_path (str): Path to the directory containing the geojson files of the annotations.\n",
    "        dataset_path (str): Path to the dataset's train, val, or test directories.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Description:\n",
    "        This function iterates over each image in the dataset split and converts the corresponding geojson\n",
    "        annotations to JSON format, with pixel coordinates calculated using GDAL. It creates a dictionary\n",
    "        containing image file information and a regions dictionary storing the asset footprints with their\n",
    "        respective shape attributes. The resulting JSON file is saved as \"nso_with_empty_annotations.json\" in the dataset path.\n",
    "        Images with no annotation have \"regions= {}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an empty dictionary to store the training/test/val set of annotations and their pixel coordinates\n",
    "    dataset_dict = {}\n",
    "\n",
    "    # Loop over each image in the dataset split\n",
    "    for file in tqdm(dataset_split, desc=f\"Creating JSONs for Detectron2 on {dataset_path}\", ncols=150, bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "        file_path = os.path.join(small_tiles_path, file)\n",
    "        img_id = file.split(\".tif\")[0]\n",
    "        geojson_image = os.path.join(geojson_path, f\"{img_id}.geojson\")\n",
    "\n",
    "        try:\n",
    "            # Not all tiles have annotations, thus:\n",
    "            if os.path.exists(geojson_image):\n",
    "\n",
    "                # Load the geojson\n",
    "                with open(geojson_image) as f:\n",
    "                    gj = json.load(f)\n",
    "\n",
    "                # Create a dictionary to store the regions (annotations spatial features) for the image\n",
    "                regions = {}\n",
    "                num_features = len(gj[\"features\"])\n",
    "\n",
    "                # Open the image with gdal to get pixel size and origin\n",
    "                gdal_image = gdal.Open(file_path)\n",
    "\n",
    "                # Get the pixel width and height and the origin coordinates\n",
    "                pixel_width, pixel_height = gdal_image.GetGeoTransform()[1], gdal_image.GetGeoTransform()[5]\n",
    "                originX, originY = gdal_image.GetGeoTransform()[0], gdal_image.GetGeoTransform()[3]\n",
    "\n",
    "                # Loop over each feature in the image\n",
    "                for i in range(num_features):\n",
    "\n",
    "                    # Get the polygon points for the feature\n",
    "                    points = gj[\"features\"][i][\"geometry\"][\"coordinates\"][0]\n",
    "\n",
    "                    # Save asset type in the dictionary if it exists, else use a default or skip\n",
    "                    asset_type = gj[\"features\"][i][\"properties\"].get(\"type\", \"unknown\")\n",
    "\n",
    "                    # If there is only one point, unwrap it\n",
    "                    if len(points) == 1:\n",
    "                        points = points[0]\n",
    "\n",
    "                    # Empty lists to store pixel coordinates\n",
    "                    all_points_x, all_points_y = [], []\n",
    "\n",
    "                    # Convert the lat/long points to pixel coordinates by subtracting origin\n",
    "                    for j in range(len(points)):\n",
    "                        all_points_x.append(int(round((points[j][0] - originX) / pixel_width)))\n",
    "                        all_points_y.append(int(round((points[j][1] - originY) / pixel_height)))\n",
    "\n",
    "                    # Create a dictionary to store the feature footprint\n",
    "                    regions[str(i)] = {\n",
    "                        \"shape_attributes\": {\n",
    "                            \"name\": \"polygon\",\n",
    "                            \"all_points_x\": all_points_x,\n",
    "                            \"all_points_y\": all_points_y,\n",
    "                            \"category\": 0\n",
    "                        },\n",
    "                        \"region_attributes\": {\n",
    "                            \"type\": asset_type\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                dictionary = {\n",
    "                    \"file_ref\": '',\n",
    "                    \"size\": os.path.getsize(file_path),\n",
    "                    \"filename\": file.replace(\".tif\", \".png\"),\n",
    "                    \"base64_img_data\": '',\n",
    "                    \"file_attributes\": {},\n",
    "                    \"regions\": regions,\n",
    "                    \"origin_x\": originX,\n",
    "                    \"origin_y\": originY\n",
    "                }\n",
    "                dataset_dict[file.replace(\".tif\", \".png\")] = dictionary\n",
    "            else:\n",
    "                # region is empty\n",
    "                gdal_image = gdal.Open(file_path)\n",
    "                pixel_width, pixel_height = gdal_image.GetGeoTransform()[1], gdal_image.GetGeoTransform()[5]\n",
    "                originX, originY = gdal_image.GetGeoTransform()[0], gdal_image.GetGeoTransform()[3]\n",
    "\n",
    "                dictionary = {\n",
    "                    \"file_ref\": '',\n",
    "                    \"size\": os.path.getsize(file_path),\n",
    "                    \"filename\": file.replace(\".tif\", \".png\"),\n",
    "                    \"base64_img_data\": '',\n",
    "                    \"file_attributes\": {},\n",
    "                    \"regions\": {},\n",
    "                    \"origin_x\": originX,\n",
    "                    \"origin_y\": originY\n",
    "                }\n",
    "                dataset_dict[file.replace(\".tif\", \".png\")] = dictionary\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    jsons_path = os.path.join(dataset_path, \"nso_with_empty_annotations.json\")\n",
    "    with open(jsons_path, \"w\") as f:\n",
    "        json.dump(dataset_dict, f, indent=2)\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "geojson_to_json_pix_coords(train, small_tiles_path, geojson_path, train_path)\n",
    "geojson_to_json_pix_coords(test, small_tiles_path, geojson_path, test_path)\n",
    "geojson_to_json_pix_coords(val, small_tiles_path, geojson_path, val_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single via_region_data training dataset => can be skipped if only one .json file.=>But then change file name\n",
    "\n",
    "for d in [\"train\", \"test\", \"val\"]:\n",
    "    jsons = [os.path.join(path, d, \"nso_with_empty_annotations.json\")]\n",
    "    result = {}\n",
    "    for file in jsons:\n",
    "        with open(file, \"r\") as f:\n",
    "            loaded = json.load(f)\n",
    "            \n",
    "        #https://realpython.com/iterate-through-dictionary-python/\n",
    "        for key, value in loaded.items():\n",
    "            result[key] = value\n",
    "    via_region_p = os.path.join(path, d, \"via_region_data_with_empty_annotations.json\")\n",
    "    with open(via_region_p, \"w\") as file:\n",
    "        json.dump(result, file)\n",
    "        \n",
    "    print(f\"Done creating JSONs {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check is the regions are well writen\n",
    "\n",
    "train = \"../Satellite/train/via_region_data_with_empty_annotations.json\"\n",
    "val =\"../Satellite/val/via_region_data_with_empty_annotations.json\"\n",
    "test = \"../Satellite/test/via_region_data_with_empty_annotations.json\"\n",
    "pths = [train, val, test]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for path in pths:\n",
    "    df = pd.read_json(path, orient='index')\n",
    "    dfs.append(df)\n",
    "\n",
    "train_df = dfs[0]\n",
    "val_df = dfs[1]\n",
    "test_df = dfs[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c198d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Prints one small tile (Valencia) to see what it looks like\n",
    "\n",
    "def check_tile_metadata(tile_path):\n",
    "    with rasterio.open(tile_path) as src:\n",
    "        print(f\"CRS: {src.crs}\")\n",
    "        print(f\"Bounds: {src.bounds}\")\n",
    "        print(f\"Resolution: {src.res}\")\n",
    "        print(f\"Number of bands: {src.count}\")\n",
    "        \n",
    "        # Plotting the tile for visual inspection\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        show(src, title=\"Small Tile Inspection\")\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "check_tile_metadata('../Satellite/small_tiles_test/3_Valencia_0_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Prints one big tile (Valencia) with annotations boundaries and the labels of the classes \n",
    "\n",
    "\n",
    "def plot_raster_with_corrected_annotations(raster_path, annotations_path, label_column):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        #raster_crs = 'EPSG:3857' If you get an error caused by rasterio force the coordinate system\n",
    "        raster_crs = src.crs\n",
    "        annotations = gpd.read_file(annotations_path)\n",
    "        \n",
    "        # Check if annotation CRS matches raster CRS and transform if not\n",
    "        if annotations.crs != raster_crs:\n",
    "            annotations = annotations.to_crs(raster_crs)\n",
    "        \n",
    "        # Calculating centroids for placing labels\n",
    "        annotations['centroid'] = annotations.geometry.centroid\n",
    "        centroids = annotations.set_geometry('centroid')\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        show(src, ax=ax, title=\"Tile with Corrected Annotations\")\n",
    "        annotations.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=2)\n",
    "        \n",
    "        # Adding labels\n",
    "        for idx, row in centroids.iterrows():\n",
    "            ax.annotate(text=row[label_column], xy=(row['centroid'].x, row['centroid'].y),\n",
    "                        horizontalalignment='center', fontsize=9, color='white')\n",
    "        \n",
    "        # Display CRS and bounds information\n",
    "        print(f\"Raster CRS: {raster_crs}\")\n",
    "        print(f\"Annotations CRS: {annotations.crs}\")\n",
    "        print(f\"Raster bounds: {src.bounds}\")\n",
    "        print(f\"Annotations bounds: {annotations.total_bounds}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "#Plot \n",
    "plot_raster_with_corrected_annotations('../Satellite/big_tiles_test/Valencia.tif', '../Satellite/geojsons_test/Valencia.geojson', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shows one big tile split into smaller tiles to show how tiling works \n",
    "\n",
    "def parse_coordinates(filename):\n",
    "    \"\"\"Extract numerical parts from the filename for sorting.\"\"\"\n",
    "    numbers = re.findall(r'\\d+', filename)\n",
    "    if numbers:\n",
    "        return tuple(map(int, numbers))  # Convert strings to integers\n",
    "    return (0, 0)\n",
    "\n",
    "def check_all_valencia_tiles(tiles_directory):\n",
    "    # List all files in the directory and sort them based on their numerical coordinates\n",
    "    tile_files = [f for f in os.listdir(tiles_directory) if 'Valencia' in f and f.endswith('.tif')]\n",
    "    tile_files.sort(key=parse_coordinates)  # Sort files based on numerical coordinates\n",
    "    \n",
    "    num_tiles = len(tile_files)\n",
    "    cols = 7  # Columns depend on size of the image \n",
    "    rows = (num_tiles + cols - 1) // cols\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for idx, tile_filename in enumerate(tile_files):\n",
    "        tile_path = os.path.join(tiles_directory, tile_filename)\n",
    "        with rasterio.open(tile_path) as src:\n",
    "            show(src, ax=axs[idx], title=tile_filename)\n",
    "\n",
    "    for ax in axs[len(tile_files):]:  # Ensure you hide exactly the unused axes\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "tiles_directory = '../Satellite/small_tiles_test'\n",
    "check_all_valencia_tiles(tiles_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Create two dataframes, one contains of all the tiles with annotation and one of all the tiles without annotation\n",
    "\n",
    "\n",
    "def check_annotations_in_tiles(tiles_directory, annotations_directory):\n",
    "    annotation_files = [f for f in os.listdir(annotations_directory) if f.endswith('.geojson')]\n",
    "    tiles_with_annotations = []\n",
    "    tiles_without_annotations = []\n",
    "    \n",
    "     # Suppress the CRS mismatch warning\n",
    "    warnings.filterwarnings(\"ignore\", message=\"CRS mismatch between the CRS of left geometries and the CRS of right geometries.\")\n",
    "\n",
    "\n",
    "    # Process each tile\n",
    "    for tile_filename in os.listdir(tiles_directory):\n",
    "        if tile_filename.endswith('.tif'):\n",
    "            tile_path = os.path.join(tiles_directory, tile_filename)\n",
    "            with rasterio.open(tile_path) as tile:\n",
    "                annotations_found = False\n",
    "\n",
    "                # Loop through each annotation file\n",
    "                for annotation_filename in annotation_files:\n",
    "                    annotations_path = os.path.join(annotations_directory, annotation_filename)\n",
    "                    annotations = gpd.read_file(annotations_path)\n",
    "                    \n",
    "                    #print(str(annotations.crs))\n",
    "                    #print(str(tile.crs))\n",
    "                    if annotations.crs != tile.crs:\n",
    "                        if tile.crs == 'LOCAL_CS[\"WGS 84 / Pseudo-Mercator\"]': \n",
    "                            value = 'EPSG:3857'\n",
    "                        else: \n",
    "                            value = tile.crs\n",
    "                        transformed_annotations = annotations.to_crs(value)\n",
    "                    else:\n",
    "                        transformed_annotations = annotations\n",
    "\n",
    "                    tile_geom = box(*tile.bounds)\n",
    "                    tile_gdf = gpd.GeoDataFrame([1], geometry=[tile_geom], crs=tile.crs)\n",
    "                    intersection = gpd.overlay(transformed_annotations, tile_gdf, how='intersection')\n",
    "\n",
    "                    if not intersection.empty:\n",
    "                        annotations_found = True\n",
    "                        break  # Stop checking other annotation files if one match is found\n",
    "\n",
    "                if annotations_found:\n",
    "                    tiles_with_annotations.append(tile_filename)\n",
    "                else:\n",
    "                    tiles_without_annotations.append(tile_filename)\n",
    "\n",
    "    # Create DataFrames from the lists\n",
    "    df_annot = pd.DataFrame(tiles_with_annotations, columns=['Tile_Name'])\n",
    "    df_empty_annot = pd.DataFrame(tiles_without_annotations, columns=['Tile_Name'])\n",
    "\n",
    "    # Optionally, print the DataFrames\n",
    "    print(\"Tiles with annotations:\")\n",
    "    print(df_annot)\n",
    "    print(\"Tiles without annotations:\")\n",
    "    print(df_empty_annot)\n",
    "\n",
    "    return df_annot, df_empty_annot\n",
    "\n",
    "# Define your paths\n",
    "tiles_directory = '../Satellite/small_tiles'\n",
    "annotations_directory = '../Satellite/geojsons'\n",
    "\n",
    "# Run the function and store DataFrames\n",
    "df_annot, df_empty_annot = check_annotations_in_tiles(tiles_directory, annotations_directory)\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "\n",
    "print(\"DataFrame with no annotations:\")\n",
    "print(df_empty_annot.head())\n",
    "\n",
    "print(\"\\nDataFrame with annotations:\")\n",
    "print(df_annot.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use this in case of class imbalance / data scarcity and creates undersampling\n",
    "### I did not use this but would definetly try it in future research\n",
    "\n",
    "# ### Creates three different sizes of training batches\n",
    "# train_path = \"../Satellite/train\"\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# test_sizes = [0.25, 0.5, 0.75]\n",
    "# train_dic = {}\n",
    "\n",
    "# for perc in test_sizes:\n",
    "#     train_name = f\"train_{int((1 - perc) * 100)}\"\n",
    "#     test_name = f\"test_{int((perc) * 100)}\"\n",
    "    \n",
    "#     # Split the DataFrame into training and test sets\n",
    "#     train_set, test_set = train_test_split(df_empty_annot, test_size=perc, random_state=42)\n",
    "    \n",
    "#     # Reset index before concatenation\n",
    "#     train_set.reset_index(drop=True, inplace=True)\n",
    "#     df_annot_reset = df_annot.reset_index(drop=True)\n",
    "    \n",
    "#     train_dic[train_name] = pd.concat([df_annot_reset, train_set], ignore_index=True)\n",
    "    \n",
    "#     # Convert to JSON\n",
    "#     data = train_dic[train_name].to_json(orient='index')\n",
    "#     with open(os.path.join(train_path, f\"via_region_data_{train_name}_empty_annotations.json\"), \"w\") as outfile:\n",
    "#         outfile.write(data)\n",
    "\n",
    "# print(f\"train_25: {len(train_dic['train_25'])}, train_50 {len(train_dic['train_50'])}, train_75: {len(train_dic['train_75'])}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71031dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Checks for data set distribution \n",
    "\n",
    "# Define the directory where your JSON files are located\n",
    "directory = '../Satellite/'\n",
    "\n",
    "# Define the categories and initialize a dictionary to store counts\n",
    "categories = ['container', 'oil_gas', 'raw', 'refinery', 'roro', 'warehouse']\n",
    "counts = {category: 0 for category in categories}\n",
    "\n",
    "# Function to count entries in a JSON file\n",
    "def count_entries(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        return len(data)\n",
    "    \n",
    "# Loop through directories \n",
    "subdirs = ['train', 'test', 'val']\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(directory, subdir)\n",
    "    if os.path.exists(subdir_path):\n",
    "        print(f\"\\nCounts for {subdir.capitalize()}:\")\n",
    "        for category in categories:\n",
    "            file_name = f'via_region_data_{category}_annotations.json'\n",
    "            file_path = os.path.join(subdir_path, file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                counts[category] = count_entries(file_path)\n",
    "            else:\n",
    "                counts[category] = 0\n",
    "                print(f\"File not found: {file_name}\")\n",
    "        for category, count in counts.items():\n",
    "            print(f\"{category.capitalize()}: {count} entries\")\n",
    "    else:\n",
    "        print(f\"Subdirectory not found: {subdir_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27042f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
